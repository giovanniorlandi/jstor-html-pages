
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>1235</title>
    </head>
    <body>
        <h1>1235</h1>
        <p><strong>URL:</strong> <a href="http://doi.org/10.5194/gmd-5-1589-2012">http://doi.org/10.5194/gmd-5-1589-2012</a></p>
        <p><strong>Full Text:</strong></p>
        <pre>['Geosci. Model Dev., 5, 1589–1596, 2012\nwww.geosci-model-dev.net/5/1589/2012/\ndoi:10.5194/gmd-5-1589-2012\n© Author(s) 2012. CC Attribution 3.0 License.\nGeoscientific\nModel Development\nCoupling technologies for Earth System Modelling\nS. Valcke1, V. Balaji2, A. Craig3, C. DeLuca4, R. Dunlap5, R. W. Ford6, R. Jacob7, J. Larson8, R. O’Kuinghttons4,\nG. D. Riley9, and M. Vertenstein3\n1CERFACS, Sciences de l’Univers au CERFACS, URA1875, 42 Av. G. Coriolis, 31057 Toulouse Cedex 01, France\n2Princeton University, Princeton, NJ, 08544, USA\n3National Center for Atmospheric Research, 1850 Table Mesa Drive, Boulder, CO, 80305, USA\n4NOAA/CIRES 325 Broadway, Boulder, CO, USA\n5College of Computing, Georgia Tech, 266 Ferst Drive, Atlanta, GA, USA\n6STFC Daresbury Laboratory, Warrington, WA4 4AD, UK\n7Mathematics and Computer Science Division, Argonne National Laboratory, 9700 South Cass Avenue, Argonne,\nIllinois 60439, USA\n8Mathematical Sciences Institute, The Australian National University, Canberra, ACT 0200, Australia\n9School of Computer Science, University of Manchester, Oxford Road, Manchester M13 9PL, UK\nCorrespondence to: S. Valcke (valcke@cerfacs.fr)\nReceived: 29 June 2012 – Published in Geosci. Model Dev. Discuss.: 25 July 2012\nRevised: 12 November 2012 – Accepted: 13 November 2012 – Published: 21 December 2012\nAbstract. This paper presents a review of the software currently\nused in climate modelling in general and in CMIP5 in\nparticular to couple the numerical codes representing the different\ncomponents of the Earth System. The coupling tech-\nnologies presented show common features, such as the ability\nto communicate and regrid data, and also offer different\nfunctions and implementations. Design characteristics of the\ndifferent approaches are discussed as well as future challenges\narising from the increasing complexity of scientific\nproblems and computing platforms.\n1 Introduction\nModel coupling is essential for realizing multi-physics simulations\nbased on two or more computing applications. An\nEarth System Model (ESM) is a quintessential example of\na coupled model, which involves several interacting components\nsimulating the atmosphere, oceans, land, and sea ice.\nThe software that links together these model components\nis called a “coupler”. Although their implementations differ\nvastly, couplers used in the geophysical community typically\ncarry out similar functions such as managing data transfer\nbetween two or more components, interpolating the coupling\ndata between different grids, and coordinating the execution\nof the constituent models. In general, coupling data must be\nregridded and passed between the components subject to different\nconstraints such as conservation of physical quanti-\nties, stability of the flux exchange numerics, consistency with\nphysical processes occurring near the component surface,\netc. In addition, computational efficiency of the coupling on\nparallel hardware is of course required. This paper provides a\nreview and a short comparative analysis of the main coupling\ntechnologies currently used in Earth System Modelling.\n2 The Earth System Modelling Framework\nThe Earth System Modelling Framework (ESMF,\nwww.earthsystemmodeling.org) is open source software for\nbuilding modelling components and coupling them together\nto form weather, climate, coastal, and other applications. It\nis used and managed by a consortium of US agencies.\nESMF is comprised of a superstructure of coupling tools\nand component wrappers with standard interfaces, and an\ninfrastructure of utilities for common functions, including\ncalendar management, message logging, grid transformations,\nand data communications (Hill et al., 2004). Infrastruc-\nture utilities, including a tool for generation of interpolation\nweights, can be used independently from the superstructure.\nPublished by Copernicus Publications on behalf of the European Geosciences Union.\n', '1590 S. Valcke et al.: Coupling technologies for Earth System Modelling\nESMF offers two kinds of component wrappers: a Gridded\nComponent which is associated with a physical domain,\nand a Coupler Component for transforming and transferring\ndata between Gridded Components. ESMF components exchange\ninformation with other components through state ob-\njects, which contain representations of physical fields.\nESMF enables components to run sequentially, concurrently,\nor in mixed mode. Components can be nested. Appli-\ncations usually run with all components linked into a single\nexecutable program, but there is also support for running separate\ncomponents as multiple executables or Web services.\nESMF can couple components written in Fortran or C, and it\nincludes a Python interface to regridding functions. Its component\nwrappers may be layered on top of other coupling\ntechnologies (e.g. MCT, FMS).\nGeneration of interpolation weights and their application\nvia sparse matrix multiplication are both implemented as parallel\noperations. ESMF supports first order conservative, bi-\nlinear, and a higher-order finite element-based patch recovery\nmethod for remapping in 2-D and in some cases 3-D.\nLogically rectangular and unstructured grids are both supported.\nThere is a range of options with respect to masking\nand handling of poles and unmapped points. The remapping\nsystem is flexible and modular; the calculation of interpolation\nweights can be performed either during a model run or\noffline, and the application of weights can be made as a separate\ncall.\nFor the most part, ESMF methods do not modify user\ndata numerically, and thus have no effect on the bit-for-bit\ncharacteristics of the model code. However, in the case of\nthe sparse matrix multiplication used to apply interpolation\nweights, user data is directly manipulated by ESMF. In order\nto help users with the implementation of their bit-for-bit\nrequirements, while also considering the associated performance\nimpact, the ESMF sparse matrix implementation pro-\nvides three levels of bit-for-bit support. The strictest level\nensures that the numerical results are bit-for-bit identical,\neven when executing across different numbers of processors.\nIn the relaxed level, bit-for-bit reproducibility is guaranteed\nwhen running across an unchanged number of processors,\nwhile the lowest level makes no guarantees about bit-for-bit\nreproducibility. The lowest level provides the greatest performance\npotential for those cases where numerical round-off\ndifferences are acceptable.\nMetadata is an important aspect of model documentation\nand interoperability. Methods of the ESMF Attribute class\ncan be used to store, aggregate and output model metadata.\nMetadata is organised into packages, following community\nconventions such as the Climate and Forecast conventions\n(see cf-pcmdi.llnl.gov), ISO standards, and the METAFOR\nCommon Information Model (Lawrence et al., 2012). Metadata\npackages can also be customised.\nIn order to adopt ESMF, modelers arrange their code as a\nset of Gridded Components and Coupler Components, and\nthen split these components into standard ESMF methods\n(initialise, run, and finalise, each of which may have multiple\nphases). The next steps are to wrap native model data\nstructures with ESMF data structures, and then register components\nwith the framework. These wrapped components can\nthen be called by a driver, which can be user-customised, to\nform a coupled application.\nESMF accommodates many implementation and sequencing\noptions but using the component wrappers alone does\nnot guarantee that components will be interoperable. In order\nto increase the level of interoperability among ESMF-\nbased systems, a collaboration led by US operational weather\ncentres has introduced conventions and generic templates for\ndrivers, components, mediators, and simple connectors. This\n“NUOPC Layer” (National Unified Operational Prediction\nCapability) is scheduled for public release in early 2013.\nTiming results for a variety of codes show that the sparse\nmatrix multiply and basic component wrappers scale to tens\nof thousands of processors. Grid remapping and parallel\ncommunications are also fast and scalable. The framework\nis very robust and is exhaustively tested nightly on 24+ platforms\nusing a suite of over 4000 tests and examples. Since it\nis a large package that encompasses many functions and features,\nusers can expect a significant learning curve to master\nthe software.\nESMF tools were used in two CMIP5 models: NASA\nGEOS-5 and CCSM4/CESM1. The GEOS-5 model uses\nESMF throughout and is structured as a deeply nested hierarchy\nof ESMF components. In CCSM4/CESM1, higher\norder interpolation weights generated by the ESMF offline\ntool were used to significantly reduce interpolation noise\nwhen mapping wind stress from atmosphere to ocean, relative\nto the bilinear method used previously. Newer versions\nof CESM use weights generated by the ESMF tool extensively\nfor reasons of speed, accuracy, and ability to handle\nmany types of grids. ESMF component interfaces are now\nsupported in CESM as well. In all, ESMF is used for coupling\nin about 12 different modelling systems, totaling about\n85 different components (see www.earthsystemmodeling.\norg/components/). ESMF regridding utilities are also being\nused within data analysis and visualization packages such as\nthe NCAR Command Language (NCL) and the Ultrascale\nVisualization – Climate Data Analysis Tools (UV-CDAT).\nESMF aims to address broader issues of interoperability\nas well as the mechanics of coupling. Future plans include\nextending ESMF functionality, addressing next generation\ncomputing challenges, and integrating ESMF components\ninto science gateways that catalog and integrate diverse resources.\n3 The new CPL7 coupler designed for CCSM4 and\nCESM1\nThe Community Climate System Model (CCSM) development\nis based at the National Center for Atmospheric\nGeosci. Model Dev., 5, 1589–1596, 2012 www.geosci-model-dev.net/5/1589/2012/\n', 'S. Valcke et al.: Coupling technologies for Earth System Modelling 1591\nResearch (NCAR) in Boulder, Colorado, USA. CCSM is a\nstate-of-the-art global climate model consisting of four fundamental\nphysical components: an atmosphere model, an\nocean model, a land surface model, and a sea ice model. In\naddition, a coupler (or driver) is used to exchange boundary\ndata between the components and to coordinate the time evolution\nof the physical models. CCSM is used to understand\nthe Earth’s global climate system, to predict the effects of climate\nchange, and to understand past climates. It is developed\nas a high performance computing application but is used on\na wide variety of platforms. The Community Earth System\nModel (CESM) is an extension of CCSM that includes an\nadditional land-ice component, a higher altitude atmosphere\nmodel option, land and ocean biogeochemistry capabilities,\nand an atmospheric chemistry model.\nPrior to CCSM4, CCSM system components ran concurrently\nas separate executables on distinct hardware proces-\nsors and a separate coupler mediated communication, performed\ngrid interpolation, and implicitly handled time inte-\ngration. With the CCSM4 release in 2010, a completely new\napproach to coupling climate components was taken within\nCCSM (Craig et al., 2012). CCSM4 is a single executable\nimplementation that contains a top-level driver and components\ncoupled via standard init/run/finalise interfaces. Indi-\nvidual components in CCSM4 can be laid out on processors\nin relatively arbitrary ways such that components can be run\non identical or independent hardware processors. The toplevel\ndriver that runs on all processors controls the processor\nlayout and time sequencing of the components. A separate\ncoupler component, which can run on a subset of all the processors,\nstill exists in the system to regrid and/or merge cou-\npling fields and carry out other coupler functions.\nComponents in CCSM4 are parallelised with MPI and\nOpenMP. The CCSM4 driver/coupler uses Model Coupling\nToolkit (MCT, see Sect. 6) datatypes and methods extensively.\nMapping weights are generated offline to ensure qual-\nity and reproducibility. By default, the CESM coupling operations\nproduce roundoff level differences when processor\ncounts or decompositions are varied, but an optional switch\nenforces bit-for-bit reproducibility when desired at some performance\ncost. A new parallel I/O (PIO) library is being used\nand offers improved I/O performance particularly in the area\nof memory scaling.\nThe new implementation improves performance because\nof greater flexibility in laying out components on hardware\nprocessors compared to the prior concurrent-only CCSM3\nsystem. CCSM4 can run on a single processor without MPI\nbut is also highly memory and performance scalable for runs\nat high resolution. The scaling of the CCSM4 coupler has\nbeen evaluated at different resolutions and on different hardware\nplatforms on up to 10 000 processors. FLOP intensive\nkernels scale linearly across all processor counts, resolutions,\nand machines. Memory intensive operations scale linearly at\nlower processor counts, but the scaling flattens out at higher\nprocessor counts as the number of gridcells per processor\ndecreases below a few hundred. Communication-dominated\nkernels tend to scale sub-linearly at lower processor counts,\nand scaling tends to drop off above about 1000 processors.\nScaling performance for communication dominated kernels\nis highly dependent on the machine and resolution. Overall,\nthe improvements in the memory and performance scaling\ncapability of the CCSM4 coupler compared to CCSM3 are\nsignificant.\nThe model is now being run at global resolutions of around\none tenth of a degree on tens of thousands of processors,\nand several thousand years worth of CMIP5 simulations have\nbeen carried out at varied resolutions and on many different\nhardware platforms.\n4 The GFDL Flexible Modelling System\nComponent-based design of model codes supposes defining\nstandard component interfaces (SCI). The Flexible Mod-\nelling System (FMS) coupler is a domain-specific SCI: it is\nwritten quite narrowly to support ESMs and recognises only\na few components: an atmosphere, an ocean surface including\nthe sea ice, a land surface, and an ocean. Any other com-\nponent inherits a grid from these, e.g. atmospheric physics\nand chemistry from the atmosphere; terrestrial biosphere,\nriver and land ice from the land surface; marine biogeochemistry\nfrom the ocean. In the FMS SCI, there are “slots” for\neach of the specific components listed above. Components\nmust be “wrapped” in FMS-specific data structures and procedure\ncalls.\nThe FMS coupler is designed to address the question of\nhow different components of the Earth System are discretised,\neach one making independent discretisation choices\nappropriate to its particular physics. In an atmospheric\nmodel, vertical diffusion is generally treated implicitly and\nstability is enhanced by computing the flux at the surface implicitly\nalong with the diffusive fluxes in the interior. Simul-\ntaneously, land or ocean surfaces with vanishingly small heat\ncapacity should be allowed. Therefore, the vertical diffusion\nof temperature in a coupled atmosphere–land system may\nlead to a tridiagonal matrix inversion which can be solved\nrelatively efficiently using an up–down sweep, with the particularity\nthat some of the layers are in the atmosphere and\nothers are in the land. Moreover, if the components are on\nindependent grids, the key flux computation at the surface is\na physical process that must be modelled on the finest possible\ngrid. Thus, the “exchange grid” (Balaji et al., 2006) on\nwhich this computation is performed in FMS emerges as an\nindependent component for modelling the surface boundary\nlayer.\nA grid is defined as a set of cells created by edges joining\npairs of vertices defined in a discretisation. Given two grids,\nan exchange grid is the set of cells defined by the union of all\nthe vertices of the two parent grids. Quantities being transferred\nfrom one parent grid to the other are first interpolated\nwww.geosci-model-dev.net/5/1589/2012/ Geosci. Model Dev., 5, 1589–1596, 2012\n', '1592 S. Valcke et al.: Coupling technologies for Earth System Modelling\nonto the exchange grid and then averaged onto the receiving\ngrid. The general procedure for solving the vertical diffusion\nis thus split into separate up and down steps. Vertically diffused\nquantities are partially solved in the atmosphere and\nthen handed off to the exchange grid, where fluxes are computed.\nThe land or ocean surface models recover the values\nfrom the exchange grid and continue the diffusion calculation\nand return values to the exchange grid. The computation\nis then completed in the up-sweep of the atmosphere. This\nfeatures is key in the design of the FMS coupler.\nThe FMS coupled modelling system also includes a parallel\nensemble adjustment Kalman filter for data assimilation\n(Zhang et al., 2005) in which ensemble members are treated\nas concurrent components.\nSince coupling involves parallel floating-point (FP) calculations,\none key concern is that of reproducibility across dif-\nferent parallel decompositions, since FP parallel arithmetic\noperations can sometimes be non-associative. Ensuring bitwise\nreproducibility across the FMS coupler involves a cost\nwhich can be as high as 10 % on some systems. Users have\nthe option of requiring reproducibility but this feature can\nbe turned off if the entire run will be done without changing\nparallel decomposition.\nThe FMS coupler has been shown to scale up to O(10 000)\nprocessors with fast surface processes coupling every atmospheric\ntime step (typically ∼ 15 min) and slow processes\ncoupling every ocean time step (typically 1 h). FMS has\nbeen active for over a decade. Its feature list and its performance\nstill remain state-of-the-art. The versatility of FMS is\nseen in GFDL’s approach to CMIP5. GFDL has submitted\nfour streams of modelling results to CMIP5: these are the\nCM3 model (Donner et al., 2011), which includes interactive\naerosol chemistry for control, historical and projection runs;\ntwo Earth System Models ESM2M and ESM2G for the carbon\ncycle runs (Dunne et al, 2012); high-resolution “time-\nslice” experiments using the HiRAM-C180 and HiRAMC360\nmodels (Zhao and Held, 2012); and near-term pre-\ndiction experiments using a sophisticated ensemble coupled\ndata assimilation (ECDA) system (Zhang et al., 2007) and\nthe CM2.1 model. All of the models are built using different\ncombinations of choices of atmospheric dynamical cores, atmospheric\nphysics packages, ocean models and the ECDA\nsystem, all of which are available as FMS components.\n5 The OASIS3 coupler\nThe development of the OASIS coupler started in 1991 at\nCERFACS. The first design focussed on flexibility (easy\nchange of coupling parameters) and low intrusiveness (components\nremain almost unchanged with respect to their stan-\ndalone mode). The OASIS3 coupler (Valcke, 2006, 2012)\nis the direct evolution of these first developments. Since\n2007, OASIS3 is developed and supported thanks to an active\ncollaboration between CERFACS and the French Centre National\nde la Recherche Scientifique (CNRS).\nOASIS3 is written in Fortran and C. In a coupled system\nassembled with OASIS3, the coupler itself forms a separate\nexecutable that performs the regridding tasks. The component\nmodels remain separate executables with main charac-\nteristics, such as internal parallelisation or I/O, untouched\nwith respect to their uncoupled mode. To interact with the\nother components through the coupler, the component models\nneed to link to the OASIS3 coupling interface library.\nThe coupling interface library API includes calls to receive\nand send the coupling fields usually implemented within the\nmodel time step loop. The characteristics of the coupling exchanges,\ne.g. the corresponding target or source component\nof an exchange or the coupling frequency, are not explicitly\ndefined in the model code but in an external configuration\nfile written by the user. At run-time the coupling library and\nthe coupler perform coupling exchanges according to the information\ncontained in this file. For regridding, OASIS3 in-\ncludes the SCRIP library (Jones, 1999), adding a few specific\noptions (such as the possibility to assign the value of the\nnearest non-masked source neighbour to target grid points\nfor which the original SCRIP algorithm would not assign any\nvalue at all).\nFor each coupling field exchange, the different parts of a\ncoupling field sent by the source model processes are gathered\nby one coupler process which regrids the whole cou-\npling field and distributes it to the target model processes.\nOASIS3 can therefore be parallelised on a field-per-field basis\nin the sense that each coupler process can treat a subset of\ncoupling fields. But even in this case, reproducibility with respect\nto parallelism is not an issue as all transformations are\ndone, for each particular coupling field, on only one coupler\nprocess.\nOASIS3 success up to now can be explained by its great\nflexibility and its low intrusiveness in the component codes.\nOASIS3 is used today by about 35 different climate modelling\ngroups in Europe, Australia, Asia and North Amer-\nica. A detailed list of users can be found in Valcke (2012).\nIn particular, OASIS3 is the coupling software used in 5 of\nthe 7 European ESMs participating to CMIP5, i.e. CNRMCM5\n(Voldoire et al., 2011), IPSL-CM5 (Dufresne et al.,\n2012), CMCC-ESM (Vichi et al., 2011; Scoccimarro et al.,\n2011), EC-Earth (Hazelger et al., 2011), and MPI-ESM from\nthe Max Planck Institute. OASIS3 is also used successfully\nin a few relatively high-resolution (∼ 2/3◦) configurations\nbut its limited parallelism will eventually become a bottleneck\nin the coupled simulation. Within the framework of\nthe current EU FP7 IS-ENES (see is.enes.org) project, work\ncontinues to parallelise and extend the existing functionality\nand to establish comprehensive services around OASIS3 (see\noasis.enes.org).\nGeosci. Model Dev., 5, 1589–1596, 2012 www.geosci-model-dev.net/5/1589/2012/\n', 'S. Valcke et al.: Coupling technologies for Earth System Modelling 1593\n6 The Model Coupling Toolkit\nThe Model Coupling Toolkit, MCT (Larson et al., 2005; Jacob\net al., 2005), embodies an application-neutral approach\nfor creating coupled multi-physics models. Thus, MCT can\nbe used in diverse scientific fields. Because MCT imposes\nno architecture on the application, the developer can freely\nchoose the number of executables and model process composition.\nAn API also allows the developer to choose such\nelements as coupling data description, parallel coupling field\nexchanges, and support for parallel data transformation and\ninterpolation. The MCT design philosophy is that flexibility\nand minimal invasiveness are vital to the development of\nlong-life-cycle coupled models.\nMCT provides a Fortran-based object model for coupling\nconstruction and bindings for C++ and Python have been\ndeveloped. An MCT datatype describes the coupled system\nprocessor layout. MCT stores coupling field data in an object\nthat supports arbitrary numbers of real- and integer-valued\nfields, indexed using string tokens. A domain decomposition\ndescriptor (DDD) object uses a 1-D global index space (i.e.\na linearization) to represent multidimensional index spaces.\nParallel communication schedules are computed automatically\nfrom source and destination DDDs. Parallel data trans-\nfer is accomplished by calling paired send/receive methods\nwith data storage and communication schedule datatypes as\ninputs. MCT provides distributed storage for precomputed\ninterpolation coefficients from which it derives communication\nschedules for parallel interpolation. This operation can\noptionally be constrained to give identical results on different\nnumbers of processes. MCT assumes MPI-based paral-\nlelism but includes a small MPI-replacement library for nonparallel\napplications.\nMCT is highly portable and uses a GNU autoconf-based\nbuild system. MCT programming model derives from Fortran\n90: module use to access MCT classes and methods,\ndeclaration of variables of MCT datatypes, and invocation\nof MCT methods to perform coupling operations. To use\nMCT, the developer locates logical interaction points in the\nsubsystem models, adds code to declare and initialise MCT\ndatatypes for coupling, and inserts handshaking calls between\nmodel pairs to initialise communication schedules.\nWithin the model run method, the user inserts calls to load\nthe coupling data into MCT datatypes and calls MCT parallel\ncommunication and interpolation methods.\nThe biggest challenge in using MCT is defining linearisations\nof mesh and index spaces. Most new MCT users,\nhowever, quickly build their own parallel coupled models\nafter experimenting with the examples provided. Ease of\nuse is is the primary benefit of MCT. Its limitations are\nlack of support for computation of interpolation weights and\nfor MPI communicator construction. MCT has been the basis\nof all CCSM couplers since 2004, supporting thousands\nof model-years of coupled climate simulation. In particular,\nthe MCT-based coupler, CPL7, is being used for all of\nthe CMIP5 integrations being performed using CCSM4 and\nCESM1 (see Sect. 3). MCT has also been used to form other\ncoupled systems; a list is available on the MCT website\n(www.mcs.anl.gov/research/projects/mct/).\nExascale platforms will require refactoring key MCT portions.\nPaucity of per-core memory at exascale requires re-\nexamination of field data copying and DDD replication.\nEmploying compatible mesh representation software in all\ncomponents could eliminate field data copying. Employing\nspace-filling curves as compact virtual linearisations could\nminimise DDD replication costs. Tolerating hardware faults\nand dynamic load balance means the MCT assumption of a\nstatic processor pool must be revisited. Work is under way to\nimplement coupling in the presence of dynamically-varying\nprocessor pools. Currently, MCT supports applications on\nhundreds of thousands of processors and is well positioned\nfor future coupled model challenges.\n7 The Bespoke Framework Generator\nThe Bespoke Framework Generator (Ford et al., 2006; Armstrong\net al., 2009) (BFG, www.cs.manchester.ac.uk/cnc/\nprojects/bfg) owes its development to an analysis of the\nMet Office future coupling requirements, the requirements\nof the GENIE paleoclimate coupled model (Armstrong et al.,\n2009, www.genie.ac.uk) and Community Integrated Assessment\nSystem (CIAS, www.tyndall.ac.uk/research/cias, War-\nren et al., 2008). BFG allows the user to choose the underlying\ncoupling technology, taking coupling metadata as input\nand generating tailored (bespoke) wrapper code to be compiled\nand linked with the user’s code and the chosen coupling\nlibrary. Separating the coupler from the science code offers\nan additional layer of flexibility which can improve portability,\nperformance and maintainability, thus future-proofing\nthe code. BFG treats transformations (such as grid transformations)\nin the same way as model code. Intrinsic transfor-\nmations (such as those in OASIS3) can also be supported.\nReproducibility is an issue for coupling systems and, as BFG\ntargets existing coupling techologies, BFG relies on the support\nfor reproducibility that they provide; the wrapper code\nproduced by BFG does not itself cause any reproducibility\nissues. BFG has been designed to be generic and extensible;\nthus, BFG may be used in application domains other than\nESM (Warren et al., 2008; Delgado-Buscalioni et al., 2005).\nBFG2, the current implementation, can be run directly\nfrom the BFG portal (see bfg.cs.man.ac.uk). The BFG2\nmodel API supports models written in Fortran as a module\ncontaining subroutines or a set of subroutines, C as\na set of procedures and Python as a class with a set of\nmethods. The coupled model behaviour is specified as composition\nand deployment metadata in XML. At the lan-\nguage level, BFG2 supports passing data to and from subroutines/procedures/methods\nvia arguments and/or in-place\nput/get calls. The former approach is similar to that used by\nwww.geosci-model-dev.net/5/1589/2012/ Geosci. Model Dev., 5, 1589–1596, 2012\n', '1594 S. Valcke et al.: Coupling technologies for Earth System Modelling\nESMF, CPL7 and FMS (see Sects. 2, 3 and 4) and the latter\nto MPI, CCSM3 and OASIS3 (see Sects. 3 and 5). Since\nmodels are not main programs, BFG2 is able to map models\nfor deployment either as a single executable or as multiple\nexecutables, each containing one or more models. Coupling\nconnections can, on first use, be initialised in a variety of\nways, including from a model or a file. Coupling technologies\ncurrently supported to exchange data between models\ninclude argument passing, MPI or OASIS. The OASIS BFG\nimplementation supports the specification of grids using an\nXML representation of the Gridspec (Balaji et al., 2006) as\nwell as the use of intrinsic OASIS transformations. The work\nwas carried out on a development version of the now withdrawn\nOASIS4 (Redler et al., 2010) and efforts are ongoing\nto extend BFG to support the most recent version of OASIS,\nOASIS3-MCT (Valcke, 2012).\nBFG2 requires model (science) code to conform to some\nsimple coding rules and to be described by definition metadata.\nComposition metadata specifies how the models (and\ntransformations) are connected together and deployment\nmetadata specifies how to map the models onto the available\nhardware and software resources. BFG2 takes the metadata\nas input and generates bespoke control and communication\ncode using a Python program.\nIn conclusion, BFG isolates science code from the coupling\ninfrastructure and provides a metadata-driven code\ngeneration system to provide flexibility in model composition\nand deployment. This flexibility allows BFG to achieve\nsimilar performance to hand-written code and provides the\nopportunity for finer-grained coupling than is typical today.\nBFG is currently used within CIAS, where it is used to couple\nover 20 different Integrated Assessment model configura-\ntions. BFG is not used in any CMIP5 runs but offers a potential\nsolution for the coupling of future Earth System Models.\nSome limitations of BFG2 are, in particular, that wrapper\ncode must be regenerated whenever the metadata changes\nand that data partition metadata for use with parallel models\nis not yet supported (but will be added soon, using MCT for\nMPI implementations). BFG2 is being extended to support\nmodels written in a less modular way – in particular, model\ncodes which are main programs, codes with internal control,\nand models where the source code is not available. In the future,\nBFG2 will be extended to support ESMF and CPL7 as\ncoupler targets. In addition, the feasibility of using BFG2 to\ncouple together models that conform to different frameworks\nby generating appropriate adaptor code will be investigated.\n8 Conclusions\nThis paper provides an overview of the current coupling technologies\nused in Earth System Modelling. Since no quanti-\ntative information was collected, conclusions are limited to\nthe likely outcomes of the different design strategies. While\nthe details of the approaches vary, features of the different\ncoupling technologies typically include an ability to communicate\ndata between components, regrid data, and manage the\ntime evolution of the model integration.\nCoupling using a concurrent multiple executable approach\n(e.g. OASIS3) requires minimal modification to existing\ncomponent codes but limits the ways they can be mapped\nto hardware, which can hinder performance. Coupling via\ncomponent-level interfaces within one integrated application\n(e.g. ESMF, CPL7 or FMS) generally requires users to split\ncomponents into initialise, run, and finalise methods, and\nmay limit the places where data exchanges can happen. Although\nthis can simplify program flow, it can also affect time\nsequencing and require scientific reformulation; however, because\ncomponents can be run sequentially or concurrently,\nthere are additional opportunities for performance optimisation.\nThis “integrated” approach also enables components to\nbe nested, with multiple coupling levels. Coupling toolkits\n(e.g. MCT) are designed for a la carte use of classes and\nmethods. They allow great flexibility for building custom\nparallel coupling mechanisms, with either single or multiple\nexecutable approaches. Subsets of other coupling technologies\n(e.g. ESMF utilities) can also be used separately to solve\nsome coupling problems. Research in generative programming\n(e.g. BFG) explores potential ways to unify the differ-\nent coupling approaches. In the end, science needs both flexible\ncoupling capabilities and high performance. Both have\nbecome crucial in the last few years as coupling complexity\nand resolution have rapidly increased and these trends are\nexpected to continue in the future. Qualitative comparison\nof the performances of the different approaches is an underresearched\nproblem and there are plans in the IS-ENES2 EU\nproject, which is a follow-on of the IS-ENES project funded\nover the 2013–2017 period, to develop a benchmark suite to\naddress this issue.\nContinual improvement in coupled climate model performance\nmay become more difficult. Most of the gains in the\nlast decade came from faster hardware on a per-processor basis\nand improvements in grid decompositions, memory par-\nallelisation, and communication algorithms. Unfortunately,\nfuture generation hardware is likely to consist of orders of\nmagnitude more processors that are slower, heterogeneous,\nand with less and slower memory. Moving into the exascale\nera will require, for coupling technology as for other software,\nboth finding additional opportunities for parallelism\nand improving communication mechanisms to better overlap\ncommunication with computation.\nOver much of the past two decades, several groups have\nworked relatively independently to develop Earth System\ncoupling technology. In many aspects, those implementations\nhave converged as individuals gain experience and as\ncommon science and high performance requirements drive\nimplementations. At the same time, different scientific communities\ncontinue to benefit from fundamentally different\nsolutions. Moving forward, the community recognises the\npotential benefit of much closer collaboration especially\nGeosci. Model Dev., 5, 1589–1596, 2012 www.geosci-model-dev.net/5/1589/2012/\n', 'S. Valcke et al.: Coupling technologies for Earth System Modelling 1595\nconsidering the uncertainty of future hardware. In addition,\nthere is recognition that if future hardware requires significant\nrewrites of Earth System Models in new programming\nlanguages, an opportunity might present itself to unify coupling\napproaches and share developmental costs.\nAcknowledgements. This paper is based on the outcome of the\nWorkshop on “Coupling Technologies for Earth System Modelling:\nToday and Tomorrow” (Valcke and Dunlap, 2011) organised in\nCERFACS in 2010. We would like to thank all participants who\ntook actively part in the discussions and the EU FP7 IS-ENES\nproject (Contract GA No: 228203) for financial support.\nEdited by: D. Ham\nThe publication of this article is financed by CNRS-INSU.\nReferences\nArmstrong, C. W., Ford, R. W., and Riley, G. D.: Coupling integrated\nEarth System Model components with BFG2, Concurr.\nComp-Pract. E, 21, 767–791, doi:10.1002/cpe.1348, 2009.\nBalaji, V., Anderson, J., Held, I., Winton, M., Durachta, J., Malyshev,\nS., and Stouffer, R. J.: The Exchange Grid: a mechanism\nfor data exchange between Earth System components on independent\ngrids, in: Proceedings of the 2005 International Confer-\nence on Parallel Computational Fluid Dynamics, College Park,\nMD, USA, Elsevier, 2006.\nCraig, A. P., Vertenstein, M., and Jacob, R.: A New Flexible\nCoupler for Earth System Modeling developed for\nCCSM4 and CESM1, Int. J. High Perform. C, 26-1, 31–42,\ndoi:10.1177/1094342011428141, 2012.\nDelgado-Buscalioni, R., Coveney, P. V., Riley, G. D., and Ford,\nR. W.: Hybrid Molecular-Continuum Models under the General\nCoupling Framework, Philos. T. Roy. Soc. A, 363, 1975–1985,\n2005.\nDonner, L. J., Wyman, B. L., Hemler, R. S., Horowitz, L. W., Ming,\nY., Zhao, M., Golaz, J. C., Ginoux, P., Lin, S. J., Schwarzkopf, M.\nD., Austin, J., Alaka, G., Cooke, W. F., Delworth, T. L., Freidenreich,\nS. M., Gordon, C. T., Griffies, S. M., Held, I. M., Hurlin,\nW. J., Klein, S. A., Knutson, T. R., Langenhorst, A. R., Lee, H.C.,\nLin, Y., Magi, B. I., Malyshev, S. L., Milly, P. C. D., Naik,\nV. and Nath, M. J., Pincus, R., Ploshay, J. J., Ramaswamy, V.,\nSeman, C.J., Shevliakova, E., Sirutis, J. J., Stern, W. F., Stouffer,\nR. J., Wilson, R. J., Winton, M., Wittenberg, A. T., and Zeng, F.:\nThe dynamical core, physical parameterizations, and basic simulation\ncharacteristics of the atmospheric component AM3 of the\nGFDL Global Coupled Model CM3, J. Climate, 24, 3484–3519,\n2001.\nDufresne, J.-L., Foujols, M.-A., Denvil, S., Caubel, A., Marti, O.,\nAumont, O., Balkanski, Y., Bekki, S., Bellenger, H., Benshila,\nR., Bony, S., Bopp, L., Braconnot, P., Brockmann, P., Cadule,\nP., Cheruy, F., Codron, F., Cozic, A., Cugnet, D., de Noblet,\nN., Duvel, J.-P., Ethé, C., Fairhead, L., Fichefet, T., Flavoni,\nS., Friedlingstein, P., Grandpeix, J.-Y., Guez, L., Guilyardi, E.,\nHauglustaine, D., Hourdin, F., Idelkadi, A., Ghattas, J., Joussaume,\nS., Kageyama, M., Krinner, G., Labetoulle, S., Lahellec,\nA., Lefebvre, M-P., Lefevre, F., Levy, C., Li, Z. X., Lloyd, J.,\nLott, F., Madec, G., Mancip, M., Marchand, M., Masson, S.,\nMeurdesoif, Y., Mignot, J., Musat, I., Parouty, S., Polcher, J.,\nRio, C., Schulz, M., Swingedouw, D., Szopa, S., Talandier, C.,\nTerray, P., and Viovy, N.: Climate change projections using the\nIPSL-CM5 Earth System Model: from CMIP3 to CMIP5, Clim.\nDynam., submitted, 2012.\nDunne, J. P., John, J. G., Adcroft, A. J., Griffies, S. M., Hallberg, R.\nW., Shevliakova, E. N., Stouffer, R. J., Cooke, W., Dunne, K. A.,\nHarrison, M. J., Krasting, J. P., Levy, H., Malyshev, S. L., Milly,\nP. C. D., Phillipps, P. J., Sentman, L. T., Samuels, B. L., Spelman,\nM. J., Winton, M., Wittenberg, A. T., and Zadeh, N.: GFDL’s\nESM2 global coupled climate-carbon Earth System Models Part\nI: Physical formulation and baseline simulation characteristics, J.\nClimate, 25, 6646–6665, 10.1175/JCLI-D-11-00560.1, 2012.\nFord, R. W., Riley, G. D., Bane, M. K., Armstrong C. W., and\nFreeman, T. L.: GCF: A General Coupling Framework, Concurr.\nComp-Pract. E, 18, 163–181, 2006.\nHazeleger, W., Wang, X., Severijns, C., Stefanescu, S., Bintanja,\nR., Sterl, A., Wyser, K., Semmler, T., Yang, S., van den Hurk,\nB., van Noije, T., van der Linden, E., and van der Wiel, K.:\nEC-Earth V2.2: description and validation of a new seamless\nearth system prediction model, Clim. Dynam., 39, 2611–2629,\ndoi:10.1007/s00382-011-1228-5, 2011.\nHill, C., DeLuca, C., Balaji, V., Suarez, M., and da Silva, A.: Architecture\nof the Earth System Modeling Framework, Comput. Sci.\nEng., 6, 18–28, 2004.\nJacob, R., Larson, J., and Ong, E.: M x N Communication and Parallel\nInterpolation in Community Climate System Model Version\n3 Using the Model Coupling Toolkit, Int. J. High. Perform. C, 19,\n293–307, 2005.\nJones, P.: Conservative remapping: First- and second-order conservative\nremapping, Mon. Weather Rev., 127, 2204–2210, 1999.\nLarson, J., Jacob, R., and Ong, E.: The Model Coupling Toolkit: A\nNew Fortran90 Toolkit for Building Multiphysics Parallel Coupled\nModels, Int. J. High Perform. C, 19, 277–292, 2005.\nLawrence, B. N., Balaji, V., Bentley, P., Callaghan, S., DeLuca, C.,\nDenvil, S., Devine, G., Elkington, M., Ford, R. W., Guilyardi, E.,\nLautenschlager, M., Morgan, M., Moine, M.-P., Murphy, S., Pascoe,\nC., Ramthun, H., Slavin, P., Steenman-Clark, L., Toussaint,\nF., Treshansky, A., and Valcke, S.: Describing Earth System Simulations\nwith the Metafor CIM, Geosci. Model Dev. Discuss., 5,\n1669–1689, doi:10.5194/gmdd-5-1669-2012, 2012.\nRedler, R., Valcke, S., and Ritzdorf, H.: OASIS4 – a coupling software\nfor next generation earth system modelling, Geosci. Model\nDev., 3, 87–104, doi:10.5194/gmd-3-87-2010, 2010.\nScoccimarro, E., Gualdi, S., Bellucci, A., Sanna, A., Fogli, P. G.,\nManzini, E., Vichi, M., Oddo, P., and Navarra, A.: Effects of\nTropical Cyclones on Ocean Heat Transport in a High Resolution\nCoupled General Circulation Model, J. Climate, 24, 4368–4384,\n2011.\nwww.geosci-model-dev.net/5/1589/2012/ Geosci. Model Dev., 5, 1589–1596, 2012\n', '1596 S. Valcke et al.: Coupling technologies for Earth System Modelling\nValcke, S.: OASIS3 User Guide (prism 2-5), CERFACS Technical\nReport TR/CMGC/06/73, CERFACS, Toulouse, France, 60 pp.,\n2006.\nValcke, S.: The OASIS3 coupler: a European climate modelling\ncommunity software, Geosci. Model Dev. Discuss., 5, 2139–\n2178, doi:10.5194/gmdd-5-2139-2012, 2012.\nValcke, S. and Dunlap, R.: Report from the workshop “Coupling\nTechnologies for Earth System Modelling: Today and Tomorrow”,\nCLIVAR Exchanges, 16, 38–39, 2011.\nValcke, S., Craig, T., and Coquart, L.: OASIS3-MCT User\nGuide(OASIS3-MCT 1.0), CERFACS Technical Report\nWN/CMGC/12/49, CERFACS, Toulouse, France, 46 pp., 2012.\nVichi, M., Manzini, E., Fogli, P. G., Alessandri, A., Patara, L.,\nScoccimarro, E., Masina, S., and Navarra A.: Global and regional\nocean carbon uptake and climate change: sensitivity to\na substantial mitigation scenario, Clim, Dynam,, 37, 1929–1947,\ndoi:10.1007/s00382-011-1079-0, 2011.\nVoldoire, A., Sanchez-Gomez, E., Salas y Mélia, D., Decharme, B.,\nCassou, C.,Sénési, S., Valcke, S., Beau, I., Alias, A., Chevallier,\nM., Déqué, M., Deshayes, J., Douville, H., Fernandez, E.,\nMadec, G., Maisonnave, E., Moine, M.-P., Planton, S., SaintMartin,\nD., Szopa, S., Tyteca, S., Alkama, R., Belamari, S.,\nBraun, A., Coquart, L., and Chauvin, F.: The CNRM-CM5.1\nglobal climate model: Description and basic evaluation, Clim\nDynam., doi:10.1007/s00382-011-1259-y, in press, 2011.\nWarren, R., de la Nava Santos, S., Arnell, N. W., Bane, M., Barker,\nT., Barton, C., Ford, R., Fussel, H. M., Hankin Robin, K. S.,\nKlein, R., Linstead, C., Kohler, J., Mitchell, T. D., Osborn, T.\nJ., Pan, H., Raper, S. C. B., Riley, G., Schellnhuber, H. J., Winne,\nS., and Anderson, D.: Development and illustrative outputs of\nthe Community Integrated Assessment System (CIAS), a multiinstitutional\nmodular integrated assessment approach for mod-\nelling climate change, J. Environ. Model. Softw., 23, 592–610,\ndoi:10.1016/j.envsoft.2007.09.002, 2008.\nZhang, S., Harrison, M. J., Wittenberg, A. T., Rosati, A., Anderson,\nJ. L., and Balaji, V.: Initialization of an ENSO Forecast System\nusing a Parallelized Ensemble Filter, Mon. Weather Rev., 133,\n3176–3201, 2005.\nZhang, S., Harrison, M. J., Rosati, T., and Wittenberg, A.: System\nDesign and Evaluation of Coupled Ensemble Data Assimilation\nfor Global Oceanic Climate Studies, Mon. Weather Rev., 135,\n3541–3564, 2007.\nZhao, M. and Held, I. M.: TC-permitting GCM simulations of hurricane\nfrequency response to sea surface temperature anomalies\nprojected for the late-twenty-first century, J. Climate, 25, 2995–\n3009, doi:http://dx.doi.org/10.1175/JCLI-D-11-00313.1, 2012\nGeosci. Model Dev., 5, 1589–1596, 2012 www.geosci-model-dev.net/5/1589/2012/\n']</pre>
    </body>
    </html>
    