
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>716</title>
    </head>
    <body>
        <h1>716</h1>
        <p><strong>URL:</strong> <a href="http://doi.org/10.1155/2022/3432688">http://doi.org/10.1155/2022/3432688</a></p>
        <p><strong>Full Text:</strong></p>
        <pre>['Research Article\nRFID Data Analysis and Evaluation Based on Big Data and\nData Clustering\nLihua Lv\nSchool of Information and Technology, Zhejiang Institute of Economics and Trade, Hangzhou, Zhejiang, China\nCorrespondence should be addressed to Lihua Lv; littledrop@zjiet.edu.cn\nReceived 30 January 2022; Accepted 2 March 2022; Published 26 March 2022\nAcademic Editor: Daqing Gong\nCopyright © 2022 Lihua Lv. (is is an open access article distributed under the Creative Commons Attribution License, which\npermits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n(e era people live in is the era of big data, and massive data carry a large amount of information.(is study aims to analyze RFID\ndata based on big data and clustering algorithms. In this study, a RFID data extraction technology based on joint Kalman filter\nfusion is proposed. In the system, the proposed data extraction technology can effectively read RFID tags. (e data are recorded,\nand the KM-KL clustering algorithm is proposed for RFID data, which combines the advantages of the K-means algorithm. (e\nimproved KM-KL clustering algorithm can effectively analyze and evaluate RFID data. (e experimental results of this study\nprove that the recognition error rate of the RFID data extraction technology based on the joint Kalman filter fusion is only 2.7%.\n(e improved KM-KL clustering algorithm also has better performance than the traditional algorithm.\n1. Introduction\nSince human society entered the information age, information\ntechnology-related industries have rapidly developed, driving\nchanges in many fields such as computers, communications,\nand data storage. With the advent of this change, the main\ncontent of people’s daily work has gradually changed to the\nprocessing of various data information. With the support of\nsoftware and hardware technology, a huge amount of data in\nvarious forms has become an important part of people’s lives.\nHowever, these massive data information often have no\nobvious structural characteristics at first glance, and traditionalmethods\nand ideas are difficult to achieve the purpose of\nfully understanding and understanding them. Coupled with\nthe huge amount of data and rich types of data, traditional\nstatistical methods cannot fully extract effective information,\nand it becomes more difficult to obtain valuable information\nin the data. On top of this predicament, data mining technology\nhas received extensive attention. With the help of data\nmining technology, people can extract originally hidden,\nunknown, and valuable information from a large amount of\ndisorganized data. By using it in other fields, the value of\ninformation has been fully reflected. Data mining technology\nis a multidisciplinary field technology, which has rapidly\ndeveloped under the huge demand for information data\nprocessing technology in the past ten years.\n(is study mainly analyzes the reasonable solution to the\nproblem of time and space requirements and tries to propose\nan effective clustering scheme for RFID data. Compared\nwith previous studies, this study mainly has the following\ntwo innovations: (1) based on big data technology, the RFID\ndata extraction technology has been improved, and the RFID\ndata extraction technology based on joint Kalman filter\nfusion is proposed. (2) An improved KM-KL clustering\nalgorithm is proposed.\n1.1. RelatedWork. RFID technology is one of the important\ntechnologies of the internet of things and big data. It realizes\nthe transmission between things, and a large amount of data\nis generated during the transmission process. (ere are\nmany research studies at home and abroad. Wang and Jiang\nproposed an OCT prediction method utilizing a combination\nof order and real-time shop floor RFID data. It uses\naccurate RFID data to describe the real-time load situation\nof the job shop and tries to mine the mapping relationship\nbetween RFID data and OCT from historical data [1].\nFazzinga et al. investigated a method for interpreting RFID\nHindawi\nComputational Intelligence and Neuroscience\nVolume 2022, Article ID 3432688, 10 pages\nhttps://doi.org/10.1155/2022/3432688\n', 'data in the context of object tracking. It consists of converting\nreadings generated by RFID-tracked moving objects\ninto semantic locations on a map by exploiting some integrity\nconstraints [2]. Martinus et al. created a system for\nsupermarket shopping research that enables people to scan\nitems themselves and quickly pay.(ey focus on using RFID\ntechnology in the system [3]. Microcontrolled devices were\nstudied by Novikov et al., and their aim was to develop a\npersonal portable effective dose dosimeter with an RFID\ndata channel without built-in power supply [4]. In the\nprocess of analysis, cluster analysis is often used for massive\nRFID data. Yunoh et al. focused on the analysis of fatigue\nstrain signals based on clustering and classificationmethods.\n(ey grouped the feature extraction using the K-means\nclustering method to obtain the appropriate number of\ndatasets. (e classification process is performed by using an\nartificial neural network (ANN) for optimal pattern recognition.\nExperiments show that their algorithm is about\n92% accurate [5]. Wang et al. predict natural disasters by\nmodeling meteorological disasters. (ey clustered natural\ndisasters through the detention analysis method, summarized\nthe characteristics of each weather system, and\ndesigned disaster control projects based on this. (eir experiments\nshow that control engineering has a high cen-\ntrality with the occurrence of disasters, and engineering\nimplementation can reduce disasters [6]. Chen used\nMATLAB software to perform statistical analysis and cluster\nanalysis on the daily PM_(2.5) concentrations observed in\nShanghai in 2014. (e results show that the PM_(2.5)\nconcentration in spring and winter is higher than that in\nsummer and autumn, and the annual distribution of\nPM_(2.5) concentration is U-shaped [7]. Balik et al. compared\nhealth indicators and health expenditures in 28 Eu-\nropean Union (EU) countries, 6 EU candidate countries,\nand 3 European Free Trade Association (EFTA) countries\nusing a cluster analysis method. As a result of the cluster\nanalysis, the countries were divided into 3 clusters, the first\ncluster including Turkey had the lowest average per capita\npublic, private, and out-of-pocket health expenditure of the\nthree clusters [8]. It can be easily seen from the related\nresearch that the research on RFID technology is more on its\napplication level than on data analysis, so the cluster analysis\nfor RFID data is rarely studied.\n2. RFID Technology\nWireless identification technology, also known as radiofrequency\nidentification technology, is often referred to as\nRFID (radio-frequency identification) technology. (is\ncommunication technology originated from the identification\napplication of British fighter jets during World War\nII and has been commercialized since the 1960s. In particular,\nthe promotion and application of the US Food and\nDrug Administration (FDA) and Walmart supermarkets,\nsuch as real-time detection of fresh commodities in supermarkets,\nintelligent price tag system, inventory control\nsystem, and intelligent shopping cart, have greatly expanded\nthe application market of RFID technology in the\nworld [9, 10].\n(e RFID system is generally composed of three parts:\nreader (Reader), electronic label (Tag), and application\nsoftware. (e tag can be divided into two parts: an antenna\nand a special chip, and the chip is attached with a unique\nidentification code, indicating the basic information attached\nto the object. (e principle is that the reader\ntransmits the emitted radio-frequency signal to the electronic\ntag by means of electromagnetic or inductive cou-\npling, so as to drive the electronic tag circuit to transmit its\ninternal data to the reader. (e reader accepts and interprets\nthe relevant data in sequence and uses the software system\nfor relevant processing [11]. Radio-frequency identification\nhas the characteristics of noncontact, not affected by environmental\nfactors, a large amount of stored information,\nreadable and writable, fast recognition speed, long recognition\ndistance, and anticollision function. It can process\nmultiple radio-frequency cards at the same time, and the two\nidentification methods of RFID are shown in Figure 1:\n(e peculiar way in which RFID devices acquire data\nleads to serious uncertainty in their data [12, 13]. (ere are\nthree main reasons for the uncertainty of RFID data, namely,\nmissing reading, overreading, and dirty data. Compared\nwith the three, multireading and dirty data phenomena are\nmore contingent and less likely to occur. (e phenomenon\nof missing reading is relatively common, which is the main\nfactor leading to the uncertainty of RFID data.\n2.1. #e Principle of Antong Data Collection. Because RFID\ntechnology has the advantages of fast and real time, it also\nhas extensive analysis in the field of its data collection. (e\nRFID data acquisition system generally includes three parts:\nthe target with the electrical label, the RFID base station, and\nthe information center. (e system composition is shown in\nFigure 2:\nBased on the working principle of RFID technology, the\nRFID data acquisition system performs radio-frequency\nidentification on the target equipped with electronic tags and\ntransmits the identification data to the traffic information\ncenter. Its working principle is as follows: (1) the system\nenters the acquisition state from the dormant state and (2)\ndetermines whether to stop the acquisition by judging the\nstop flag bit of reading the radio-frequency tag. If there is no\nstop command, the system starts to read the target information.\n(3) (e RFID base station judges whether a tag\nenters the radio-frequency field through radio-frequency\ntechnology. (4) After sending the request, the mark is selected\nto confirm whether to detect the specific target. (5)\nAfter the verification is passed, the resident writer starts to\nread the tag carried by the target and returns and transmits\nthe read response data to the information center after the\noperation is successful. (e workflow of the RFID data\nacquisition system is shown in Figure 3:\n2.2. Joint Kalman Filter Fusion of RFID Technology and Coil\nTechnology. For the fusion methods of multisensor information\nof the same detection section, the fusion methods\nsuitable for dynamic traffic parameters mainly include joint\n2 Computational Intelligence and Neuroscience\n', 'filter fusion, centralized filter fusion, and neural networkbased\nfusion methods.\n(e basic structure of the joint Kalman filter adopts the\nmethod of two-stage processing of data and scattered filtering.\n(e combined filter consists of the main filter and\nseveral subfilters.\n(e structure diagram of the joint Kalman filtering algorithm\nused in this study is shown in Figure 4:\nIn the structure of the joint Kalman filter in Figure 4,\nthe subfilter first performs independent filtering, transmits\nthe filtering result to the main filter, and at the same\ntime completes the optimal fusion of the collected\ninformation.\nAmong them, the state formula and measurement formula\nof the filter are equations (1) and (2), respectively:\nXi(k) � Φ(k, k − 1) · Xi(k − 1) + Wi(k − 1), (1)\nYi(k) � Hi(k) · Xi(k) + Vi(k). (2)\nwhere Wi(k − 1) is the noise of the dynamic model, and its\ncovariance is Qi(k − 1). Vi(k) is the observation noise, and\nits covariance is Ri(k). Since the traffic volume collected by\nthe coil sensor is slightly smaller than the actual value, this\nstudy improves the joint Kalman filter. In this study, a sensor\nwith a known acquisition error is selected as the reference\ninformation Center\nbase station\nRFID data acquisition system\nTarget with electronic tag\nFigure 2: Composition of RFID traffic data collection system.\ninductive coupling\nelectronic tag\nelectronic tag\nelectromagnetic coupling\nelectronic tag\nFigure 1: RFID (radio-frequency identification) technology.\nComputational Intelligence and Neuroscience 3\n', 'sensor, and the method of the first comparison and then\nfusion is adopted. Its filtering formula is shown in equations\n(3) and (4):\nXi(k + 1) � Φ(k + 1, k) · Xi(k), (3)\npi(k + 1, k) � Φ(k + 1, k)Pi(k)ΦT\n(k + 1, k) + Qi(k). (4)\n(e main filter does not perform filtering but directly\nperforms data fusion. (e fusion method is shown in\nequations (5) and (6):\nXg � Pg \U0010ff58\nn,m\ni�1\nP\n−1\ni Xi\n⎛⎝ ⎞⎠, (5)\nPg � \U0010ff58\nn,m\ni�1\nP\n−1\ni\n⎛⎝ ⎞⎠\n− 1\n; i � 1, 2. (6)\nWhen the main filter is fused, it feeds back information\nto the subfilters according to the fusion result, where the\nfeedback factors are shown in equations (7) and (8):\nβi � p\n−1\n1 + p\n−1\n2\U0010ff10 \U0010ff11\n− 1\n· p\n−1\ni , (7)\np\n−1\ni (k) � βi · p\n−1\ng (k). (8)\nWhen the condition of Q1<Q2 is not satisfied, it can be\nseen from equations (5) to (7) that β1 + β1 � 1.\nXg � β1X1 + β2X2, that is, the fusion result Xg will take a\nvalue between X1 and X2, and change between the two\nvalues as the feedback factor changes. At the same time, the\nsubfilters are fed back according to the fusion result, and the\ninformation is redistributed to improve the fusion accuracy.\nCompared with the centralized Kalman filter and artificial\nneural network, the joint Kalman filter has the ad-\nvantages of flexible design, a simple algorithm, better fault\ntolerance, and more suitable for real-time systems.\nIn this study, the data Q1 collected by RFID technology\nand the data Q2 collected by the coil sensor are directly\ncombined with Kalman filtering (hereinafter referred to as\ngeneral fusion). At the same time, it compares the size of Q1\nand Q2 and then fuses them according to the fusion method\n(hereinafter referred to as improved fusion), as shown in\nFigure 5.\nAs can be seen from Figure 5, when the traffic volume\ncollected by the RFID technology and the coil sensor is\nsmaller than the actual value, the fusion effect is poor. When\nthe traffic volume collected by RFID technology is smaller\nthan that of the coil sensor, the traffic volume collected by\nthe coil sensor is directly taken as the fusion value. (e\nfusion curve coincides with the traffic volume change curve\ncollected by the coil sensor, which significantly improves the\naccuracy of the fusion result.\nIt can be seen from Table 1 that in dataset 1, the error of\nthe improved fusion is the same as that of the general fusion,\nso the improved fusion method in this study has certain\napplicable conditions. (at is, when the traffic volume\ncollected by RFID technology is smaller than the traffic\nvolume collected by the known small sensor or larger than\nthe traffic volume collected by the known large sensor, the\nimproved fusion proposed in this study produces better\nresults. From dataset 2 and dataset 3, it can be seen that the\nrelative error is significantly reduced by using the improved\nmethod in this study for fusion.\nY\nN\nY\nN\nN\nY\nsystem\ninitialization\nDetermine the stop bit of the RFID\ntag command\nDetermine whether the target\nenters the radio frequency range\nsend request\nSelect the target label for\nacquisition\nVerified\nTag carried by the\ntarget\nSuccessful operation\nReturn the collected RFID\ndata\nFigure 3: RFID traffic data collection system operation process.\n4 Computational Intelligence and Neuroscience\n', '3. Analysis and Result\n3.1. Cluster Analysis. Pattern recognition is to study the\nautomatic processing and interpretation of patterns\nthrough the use of mathematical techniques by computers,\nand the environment and objects are collectively\nreferred to as “patterns.” With the development of\ncomputer technology, it is possible for human beings to\nstudy the complex information processing process. An\nimportant form of the process is the recognition of the\nenvironment and objects by the living body. In the field of\npattern recognition and statistical analysis, cluster analysis\nhas always been the focus and research direction of the\nacademic community. So far, a large number of theories\nand methods have been proposed, and remarkable\nresearch results have been achieved. After a long period of\nresearch and development, clustering analysis can be\ndivided as follows [14, 15]:\n3.1.1. Partition-Based Approach. (e typical algorithm is the\nK-means algorithm and its characteristics are summarized in\nTable 1, so it will not be repeated here.\n3.1.2. Hierarchical Approach. Hierarchical clustering\nmethods use top-down splitting or bottom-up agglomeration\nto represent datasets in a hierarchical tree structure.\nEach object is first treated as a separate cluster, and these\nclusters are merged into larger and larger clusters until a\noutput result\ntime update\noptimal fusion\nmain filter\nfeedback\nSubfilter 1\nSubfilter 2\nSensor 1\nSensor 2\nSubfilter nSensor n\npublic reference\nsystem\nFigure 4: Structure diagram of joint Kalman filter.\n150\n160\n170\n180\n190\n200\n210\n220\n230\n1 2 3 4 5 6 7 8 9 10 11 12 13 1415 16 17 1819 20 21 2223 24\nnu\nm\nbe\nr\ntime\nQ1\nQ2\nfusion\nFigure 5: Traffic fusion results of the joint Kalman filter algorithm.\nTable 1: Comparison of average relative errors (unit: %).\nData Traditional RFID technology Coil sensor General fusion Improve fusion\n1 8.52 6.27 0.97 0.97\n2 10.04 4.93 7.23 4.6\n3 9.56 5.78 5.72 2.74\nAverage 9.37 5.66 4.64 2.77\nComputational Intelligence and Neuroscience 5\n', 'given end condition is met. (is clustering method is a\nbottom-up agglomerative clustering method. Divisive hierarchical\nclustering is just the opposite, where all objects are\nfirst grouped into one large cluster. (en, it is gradually\ndivided into smaller and smaller clusters according to the\nsimilarity, until the end condition is satisfied.\n3.1.3. Density-Based Methods. Density-based methods are\ndivided into different clusters according to different aggregation\ndensities in data object sets, and clusters with\nsimilar densities are divided into one cluster. However, its\ncalculation is large, usually is O(n2). In addition, based on\nthe density, the setting of parameters has a great influence on\nthe performance of the algorithm, and there is no good\nsolution at present, which mainly depends on the user’s\nexperience to select parameters.\n3.1.4. Grid-Based Approach. Grid-based clustering method\ndivides the data space into a certain number of ultrarectangular\ngrid cells according to the partition parameters,\nmaps the data objects to the corresponding grid cells, and\nthen merges the adjacent grid cells into a connected region,\nthat is, a cluster. Grid-based clustering method has good\nscalability for the size of datasets, can handle large-scale\ndatasets, and can find clusters with arbitrary shapes. Gridbased\nclustering method is usually combined with the\ndensity-based clustering method [16].\n3.1.5. Model-Based Approach. (e model-based clustering\nmethod is to use a specific model for cluster analysis and try\nto optimize the fit between the actual data and the model.\nNeural network-based clustering methods and statistical\nlearning-based clustering methods are two types of modelbased\nclustering methods. Among the neural network\nmodels commonly used in cluster analysis are the self-organizing\nmap (SOM) model [17], the adaptive resonance\ntheory model [18], and the learning vector quantization\n(LVQ) model. (e EM clustering algorithm based on the\nGaussian mixture model is a typical clustering method based\non statistical learning [19].\n3.1.6. Fuzzy Clustering Method. (e clustering method\ndescribed above can be regarded as hard clustering. Different\nfrom hard clustering, fuzzy clustering is a soft clustering\nmethod. Fuzzy C-means clustering algorithm [20] is a\npopular fuzzy clustering algorithm at present, which\ntransforms the clustering problem into an optimization\nproblem and uses an iterative method to solve it. It has a\nsimple design, good clustering performance, and wide application.\nHowever, the algorithm is sensitive to the initial\nconditions, easily falls into local optimum, requires a large\namount of computation, and has a low resolution for objects\nin the overlapping area of class boundaries.\nTable 2 summarizes the clustering methods commonly\nused at present, and the advantages and disadvantages of each\nmethod, in which NCA means to determine the number of\nclasses sensitive to initial values and outliers in advance.\n3.2. Partition-Based Clustering Algorithm—KM-KL\nAlgorithm\n3.2.1. #e Basic Concept of KM-KL Algorithm. In the traditional\npartition-based clustering algorithm, the set of\nuncertain objects is divided into given K clusters according\nto their mutual distances. (e KM-KL algorithm in this\nstudy follows the idea of the traditional partition-based\nclustering algorithm and divides the uncertain dataset O\ncontaining n uncertain objects into k clusters. (ey are\ndenoted as C1,L, Ck, respectively, use the symbol Ci as the\ncenter point of each cluster, and use the center point to\nrepresent the cluster Ci. In the partition-based clustering\nalgorithm, the following properties are included as follows:\n(1) All clusters belong to the uncertain dataset, that is\nCi⊆O(1≤ i≤ k).\n(2) Each cluster contains at least one data record, that is\nCi ≠φ.\n(3) Each data record belongs to one cluster and belongs\nto only one cluster, that is, when equation (9) is\nsatisfied, equation (10) can be satisfied:\nU\nk\ni�1Ci � O, i≠ j, (9)\nCi ∩Cj � φ. (10)\nOn the basis of traditional partition-based clustering, the\nKM-KL algorithm in this study uses KL divergence as a\nsimilarity measure.\n(e algorithm divides uncertain objects into k clusters\nand selects an optimal cluster center point for each cluster to\nminimize the sum of the overall KL divergence. In the\nclustering algorithm, in the division formed by a clustering\nprocess, the sum of the KL divergence among all objects is\nshown in the following:\nTKL � \U0010ff58\nk\ni�1\n\U0010ff50\np∈Ci\nD PCi( \U0010ff01. (11)\n(is formula is used to measure the quality of the\nclustering. (e larger the TKL value, the worse the quality of\nthis clustering is, and the smaller the TKL value is, the better\nthe quality of this clustering is. In equation (11), D(PCi)\nrepresents the KL distance from the object P to the cluster\ncenter point Ci, which is used to assign the object to each\ncluster.\n\U0010ff58\np∈Ci\nD PCi( \U0010ff01.\n(12)\nFormula (12) represents the sum of the KL distances\nfrom each object P in the cluster Ci to the cluster center point\nCi, according to which the structure of the cluster Ci can be\nadjusted.\n3.2.2. KM-KL Algorithm Description. (eKM-KL algorithm\nin this study is an extension of the K-medoids algorithm\nusing KL divergence as a similarity measure. (is algorithm\n6 Computational Intelligence and Neuroscience\n', 'clusters uncertain data on probabilistic similarity [11–22].\n(e algorithm is divided into two parts, the initial clustering\ndivision stage and the center replacement stage.\n(1) Initializing the clustering stage. In the initial clustering\nand division stage, the algorithm selects k cluster center\npoints one by one to initialize the clustering and division of\nother objects. (e first center point C1 selects the object with\nthe smallest sum of KL divergence of other objects in the\nuncertain object set O, that is, formula:\nC1 � argmin \U0010ff58\nP′∈O\nD P′P( \U0010ff01⎛⎝ ⎞⎠, (13)\n(e remaining k− 1 center points are iteratively selected.\nAt the i-th iteration, the algorithm selects the object Ci that\nminimizes TKL. When it is calculated that P′ will be allocated\nto the cluster with P as the new cluster center point, the\ncontribution of this allocation to reducing TKL is shown in\nthe following:\nmax 0,mini−1\nj�1 D P′Cj\U0010ff10 \U0010ff11 − D P′Cj\U0010ff10 \U0010ff11\U0010ff10 \U0010ff11\U0010ff10 \U0010ff11 (14)\nFrom formula (14), it can be calculated the sum of the\nreduction in TKL by the division of all unselected objects in\nthis round, which is expressed as DEC(P). In the KM-KL\nalgorithm, the center point selected by the i-th iteration is\nthe object with the largest DEC(P), that is, formula:\nCi � argmax\nP∈O C1 ,...,Ci−1{ }\n(DEC(P)).\n(15)\n(is ensures that the initial cluster center point selected\nat each time is optimal, because this selection can minimize\nthe dissimilarity between clusters. (e initial clustering\ndivision phase ends when all k cluster center points are\nselected and then enters the second phase of the algorithm,\nthe center replacement phase.\n(2) Center replacement stage. In the center replacement\nstage, the algorithm iteratively replaces the cluster center\npoints with all noncenter points in the cluster and selects the\noptimal cluster center point to improve the clustering\nquality. Every time redistribution occurs, it records the TKL\nreduction value after redistribution and selects the object\nwith the largest value as the new cluster center, as shown in\nthe following:\nPmax � argmax\nP∈O C1 ,...,Ci−1{ }\n(DEC(P)).\n(16)\nAt this time, if it is DEC(Pmax), it means that this round\nof exchange improves the clustering effect. Otherwise, the\nalgorithm ends and the final clustering is generated. (e\nexample shown in Figure 6 is used to illustrate the KM-KL\nalgorithm flow.\nAs shown in Figure 6, Figure 6(a) shows the distribution\nof the uncertain object set. Supposing that the uncertain\nobjects a1, a2, and a3 satisfy the same distribution, it forms a\ncluster. (e uncertain objects b1, b2, and b3 satisfy another\ndistribution and it forms another cluster, which is the\nclustering result of the benchmark. Figure 6(b) shows the KL\ndivergence between uncertain objects and the initialized\nTKL value. Due to the asymmetry of KL divergence, in\nFigure 6(a), for example, the divergence from a1 to a2 is not\nequal to the divergence from a2 to a1. At this time, the KMKL\nalgorithm is used to cluster the uncertain objects in\nFigure 6(a), and the input parameter k is equal to 2.\nFinally, the KM-KL algorithm forms two clusters {a1, a2,\na3} and {b2, b1, b3} with a1 and b2 as the center points,\nrespectively, and the division is the optimal division.\n3.3. Algorithm Performance Test and Effectiveness Analysis.\n(e experimental platform is configured as follows: Intel(R)\nprocessor, 2.94GHz main frequency, 2G memory, using\nWindows7 operating system. (e experimental program is\nwritten in C++ language, compiled and run on VS2010, and\nsimulated by MATLAB.\n(is study generates datasets on discrete and continuous\ndomains, respectively. In the continuous domain, an uncertain\nobject is a sample taken from a continuous distri-\nbution, and in the discrete domain, the dataset is transformed\nfrom a continuous model. (e continuous domain is discretized\nusing a meshing method, dividing each dimension\ninto two equal parts, thus dividing the d-dimensional data\nspace into the same 2D cells. (e probability of an object in a\ncertain unit is the sum of the probabilities of all sample points\nof the object in this unit. (e value range of the data is [0, 1]d.\n(ree different probability distributions will be used in this\nstudy, namely, uniform distribution, Gaussian distribution,\nand inverse Gaussian distribution, which is generated by\nGaussian distribution, as shown in Figure 7.\nTable 2: Types of clustering methods.\nType Advantage Insufficient\nBased on\npartition\nmethod\nWide application, fast convergence, incremental clustering,\nand suitability for large-scale data\nIt is necessary to determine the NCA, which is sensitive to\ninitial values and outliers, so as to find circular clusters\nHierarchy-based\nmethod\nIt does not need to determine the NCA and can find clusters\nof any shape, which is suitable for data of any attribute and\nhas strong clustering ability\nNo backtracking, no exchange of data objects between\nclasses, no full processing of large-scale data, and no\nincremental clustering\nDensity-based\nmethod\nIt does not need to determine the NCA, can find clusters of\ndifferent shapes, can detect outliers, and has good\nadaptability to large datasets\nIt is very sensitive to parameters. For datasets with uneven\ndensity distribution, the quality of clustering results is not\nhigh\nComputational Intelligence and Neuroscience 7\n', '(e experiments in this study are mainly divided into\nthree parts: first, the effect of the clustering algorithm using\nKL divergence is analyzed. Second, it shows the improvement\nof the computational efficiency of the clustering al-\ngorithm in this study after using an efficient implementation\nmethod and an improved fast Gaussian transform. Finally,\nexperiments show the scalability of the algorithm on large\ndatasets.\nIn classical partition-based and density-based clustering\nalgorithms, KL divergence and geometric distance are used\nas similarity measures, respectively, to compare clustering\nquality. Among the partition-based clustering algorithms,\nthe UK-means algorithm using geometric distance (denoted\nas UK) and the KM-KL and RKM-KL algorithms proposed\nin this study are compared. Among the density-based\nclustering algorithms, the FDBSCAN algorithm using\ngeometric distance (denoted as FD) and the DB-KL algorithm\nin this study are compared.\nIn the experiment, the base number of objects is set to\nn� 100 by default, each object contains s� 100 sample\npoints, the data dimension d� 4, and the default setting of\nclustering produces k� 6 clusters. For the density-based\nalgorithm, k is not used as a parameter, and the parameters’\ndensity threshold μ and distance radius ε need to be set here.\nAmong them, the density threshold is μ � 5 according to the\nrecommendation, and the distance radius ε is continuously\nadjusted in the experiment so that the z FD and DB-KL can\nproduce clustering results’ approximate to k clusters.\nSince the complexity of the density-based algorithm DBKL\nexponentially increases with respect to the number of\ndata objects, the algorithm is not suitable for a large number\nof data objects. (e scalability of the RKM-KL algorithm is\nmainly tested here. (e dataset defaults to 4 dimensions and\ncontains 10 clusters.\nFigures 8 and 9 show the effect of the RKM-KL algorithm\nand the RKM-KL-FGTalgorithm on the clustering quality of\nthe algorithm when the data object cardinality is large and\nthe number of object samples is large. It can be seen that\nwhen the object cardinality is large or the number of samples\nis large, the algorithm has a similar quality trend as when the\namount of data is small. Moreover, as in the previous experimental\nanalysis, the clustering quality of the RKM-KL-\nFGTalgorithm will be reduced to a certain extent compared\nto the RKM-KL algorithm.\nFigure 10 shows the running time of the RKM-KL algorithm\nand the RKM-KL-FGT algorithm when the dataset\nis large. It can be seen that the running time of the RKM-KL\nalgorithm linearly increases as the cardinality of the data\nobjects increases. (e RKM-KL-FGT algorithm uses an\nimproved fast Gaussian transform to obtain approximations,\nso the running time of the algorithm is shorter, and the\nrunning time smoothly increases with the increase in cardinality.\nHowever, since the calculation of the sum of KL-\ndivergence squarely increases with respect to the number of\nsamples of the object, the calculation time of the algorithm\nRKM-KL rapidly increases with the increase in the sample\ndata volume. While the computation time of the RKM-KLFGT\nalgorithm is almost unaffected (it grows linearly),\nobviously, RKM-KL-FGT is more effective when the amount\nof data is large.\n(a) (b) (c)\nFigure 7: (ree different distributions. (a) Evenly distribution. (b) Normal distribution. (c) Inverse Gaussian distribution.\na2\na3\na1\nb3b1\nb2\n(a)\na1 a2 a3 b1 b2 b3\n0 0.2 0.40.4a1\n0.15 0 0.7\n0.7\na2\n0.2 0 0.6\n0.6\n0.6\n0.6\na3\n0 0.4b1\n0.35b2\n0.4 0.6 0.5\n0.5\n0.5\n0.5\n0.5\n0.4\n0.4\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0.3\n0 0b3\n1.35 2 2.2 2.05 2.3 2.4TKL\n(b)\nFigure 6: Data objects and their KL divergence.\n8 Computational Intelligence and Neuroscience\n', '0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n2000 4000 6000 8000 10000\nco\nve\nra\nge\n ra\nte\ncardinal number (n)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n100 300 500 700 900\nco\nve\nra\nge\n ra\nte\nsample number (n)\nRKM-KL\nRKM-KL-FGT\nRKM-KL\nRKM-KL-FGT\nFigure 9: Coverage when using large datasets on continuous domains.\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\nru\nn \ntim\ne (\ns)\nru\nn \ntim\ne (\ns)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n2000 4000 6000 8000 10000\ncardinal number (n)\nRKM-KL\nRKM-KL-FGT\nRKM-KL\nRKM-KL-FGT\n100 300 500 700 900\nsample number (n)\nFigure 10: Running time when using large datasets on continuous domains.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n2000 4000 6000 8000 10000\nac\ncu\nra\ncy\n ra\nte\ncardinal number (n)\nRKM-KL\nRKM-KL-FGT\nRKM-KL\nRKM-KL-FGT\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n100 300 500 700 900\nac\ncu\nra\ncy\n ra\nte\nsample number (n)\nFigure 8: Accuracy when using large datasets on continuous domains.\nComputational Intelligence and Neuroscience 9\n', '4. Conclusions\n(e application of RFID technology has been relatively\nmature, and there are applications in many places. Especially\nafter these years of development, RFID technology has\nrelatively mature applications in many aspects, such as\nsupermarkets, express delivery, logistics, and other industries.\nIn this study, the massive data generated by RFID\ntechnology are analyzed and clustered to mine effective\ninformation and reduce system running time. (is study\nstarts from the relevant background of big data and computer\nscience and introduces the relevant research back-\nground of RFID technology. Subsequently, this study will\nintroduce the RFID technology in detail, from the definition\nto the calculation process. In this study, the cluster analysis is\nintroduced in detail, and an improved KM-KL algorithm is\nproposed, which is proved to be very effective. However,\nthere are also shortcomings in this study, that is, the data\nextraction is too extensive, and subsequent research can\nconduct specific analysis on the data of a certain industry,\nsuch as logistics data.\nData Availability\n(e data used to support the findings of this study are\navailable from the corresponding author upon request.\nConflicts of Interest\n(e authors declare no conflicts of interest.\nAcknowledgments\n(is research study was sponsored by the Visiting Scholar\nTeacher Professional Development Project in Colleges and\nUniversities (project number: 00058DF2014401010101). (e\nproject is thanked for the support.\nReferences\n[1] C. Wang and P. Jiang, “Deep neural networks based order\ncompletion time prediction by using real-time job shop RFID\ndata,” Journal of Intelligent Manufacturing, vol. 30, no. 3,\npp. 1303–1318, 2019.\n[2] B. Fazzinga, S. Flesca, F. Furfaro, and F. Parisi, “Using integrity\nconstraints to guide the interpretation of RFID-tra-\njectory data,” Sigspatial Special, vol. 9, no. 2, pp. 28–35, 2017.\n[3] Martinus, M. S. Wahab, Yudi, and H. Ham, “Data transmission\nusing RFID system on smart shopping carts for\ncheckout process efficiency in supermarket at Indonesia,”\nProcedia Computer Science, vol. 179, no. 2, pp. 902–912, 2021.\n[4] S. G. Novikov, A. V. Berintsev, A. S. Alekseyev, A. I. Somov,\nand V. V. Svetukhin, “Development of a personal portable\ndosimeter of an effective dose with the RFID data channel,”\nRadio Industry (Russia), vol. 28, no. 3, pp. 78–85, 2018.\n[5] M. F. M. Yunoh, S. Abdullah, M. H. M. Saad, Z. M. Nopiah,\nand M. Z. Nuawi, “K-means clustering analysis and artificial\nneural network classification of fatigue strain signals,” Journal\nof the Brazilian Society of Mechanical Sciences and Engineering,\nvol. 39, no. 3, pp. 757–764, 2017.\n[6] W. C. Wang, “Setting up evaluate indicators for slope control\nengineering based on spatial clustering analysis,” Natural\nHazards, vol. 93, no. 2, pp. 1–19, 2018.\n[7] Y. Chen, Y.Wang, M. Zhang, Z. Xu, and G. Zhang, “Temporal\nand spatial distribution of PM_(2.5) in Shanghai based on\nclustering analysis,” Chinese Journal of Environmental Engineering,\nvol. 11, no. 6, pp. 3671–3677, 2017.\n[8] P. Y. Balik, E. Demrc, andM. Konca, “Comparison of European\ncountries’ health indicators and health expenditures by clustering\nanalysis,” Ömer Halisdemir Üniversitesi İktisadi ve İdari\nBilimler Fakültesi Dergisi, vol. 14, no. 2, pp. 365–377, 2021.\n[9] A. Qubaa and S. Al-Hamdani, “Detecting abuses in archaeological\nareas using k-mean clustering analysis and UAVs/\ndrones data,” Scientific Review Engineering and Environmental\nSciences, vol. 30, no. 1, pp. 182–194, 2021.\n[10] L. A. Bulla-Cruz, L. L. Barrera, and A. Darghan, “Completelinkage\nclustering analysis of surrogate measures for road\nsafety assessment in roundabouts,” Revista Colombiana de\nEstadı́stica, vol. 44, no. 1, pp. 91–121, 2021.\n[11] Z. Lu, W. Li, Y. Tang, Z. Da, and X. Li, “Lymphocyte subset\nclustering analysis in treatment-naive patients with systemic\nlupus erythematosus,” Clinical Rheumatology, vol. 40, no. 5,\npp. 1835–1842, 2021.\n[12] K. Ansari and T. S. Bae, “Clustering analysis of seismicity in\nthe space–time–depth–magnitude domain preceding the 2016\nKumamoto earthquake, Southwestern Japan,” International\nJournal of Earth Sciences, vol. 110, no. 1, pp. 253–261, 2021.\n[13] H. Li, Y. Yang, and S. Yin, “Two λ-correlation coefficients of\nq-rung orthopair fuzzy sets and their application to clustering\nanalysis,” Journal of Intelligent and Fuzzy Systems, vol. 39,\nno. 5, pp. 1–11, 2020.\n[14] L. Close and R. Kashef, “Combining artificial immune system\nand clustering analysis: A stock market anomaly detection\nmodel,” Journal of Intelligent Learning Systems and Applications,\nvol. 12, no. 4, pp. 83–108, 2020.\n[15] H. J. Kim, “Phone scam: Developing an investigative technique\nthrough web scraping and geo-clustering analysis,”\nKorean Police Studies Review, vol. 19, no. 3, pp. 45–62, 2020.\n[16] J. F. Wang, Z. C. Fei, Q. Chang, Y. Fu, and S. Q. Li, “Energysaving\noperation of multistage stochastic manufacturing\nsystems based on fuzzy logic,” International Journal of Simulation\nModelling, vol. 18, no. 1, pp. 138–149, 2019.\n[17] A. Akramunnisa and F. Fajriani, “K-means clustering analysis\npada PersebaranTingkat pengangguran kabupaten/kota di\nSulawesi selatan,” Jurnal Varian, vol. 3, no. 2, pp. 103–112, 2020.\n[18] M. R. Hashmi, M. Riaz, and F. Smarandache, “m-Polar neutrosophic\ntopology with applications to multi-criteria decision-\nmaking in medical diagnosis and clustering analysis,” International\nJournal of Fuzzy Systems, vol. 22, no. 1, pp. 273–292, 2020.\n[19] P. J. Bardzinski, P. Walker, R. Krol, and W. Kawalec, “Simulation\nof random tagged ore flow through the bunker in a\nbelt conveying system,” International Journal of Simulation\nModelling, vol. 17, no. 4, pp. 597–608, 2018.\n[20] D. Yan, H. Cao, Y. Yu, Y. Wang, and X. Yu, “Single-objective/\nmultiobjective cat swarm optimization clustering analysis for\ndata partition,” IEEE Transactions on Automation Science and\nEngineering, vol. 17, no. 3, pp. 1633–1646, 2020.\n[21] Y. Li, H. Liu, H. O. Ramadhani et al., “Trust/RV368 Study\nGroup Genetic clustering analysis for HIV infection among\nMSM in Nigeria,” AIDS, vol. 34, no. 2, pp. 227–236, 2020.\n[22] A. J. Carrillo, I. E. Cabrera, M. J. Spasojevic, P. Schacht,\nJ. E. Stajich, and K. A. Borkovich, “Clustering analysis of largescale\nphenotypic data in the model filamentous fungus Neu-\nrospora crassa,” BMC Genomics, vol. 21, no. 1, pp. 1–22, 2020.\n10 Computational Intelligence and Neuroscience\n']</pre>
    </body>
    </html>
    