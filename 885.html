
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>885</title>
    </head>
    <body>
        <h1>885</h1>
        <p><strong>URL:</strong> <a href="http://doi.org/10.5194/se-2021-69">http://doi.org/10.5194/se-2021-69</a></p>
        <p><strong>Full Text:</strong></p>
        <pre>['1 \n \nTeaching Uncertainty: A new framework for communicating \nunknowns in traditional and virtual field experiences \nCristina G. Wilson1,2, Randolph T. Williams3, Kathryn Bateman2, Basil Tikoff3, Thomas F. Shipley2 \n1General Robotics, Automation, Sensing, and Perception Laboratory, School of Engineering and Applied Science, University \nof Pennsylvania, Philadelphia, PA, USA 5 \n2Department of Psychology, College of Arts and Sciences, Temple University, Philadelphia, PA, USA \n3Department of Geoscience, College of Letters and Science, University of Wisconsin-Madison, Madison, WI, USA \n \nCorrespondence to: Cristina G. Wilson (wilsoncg@seas.upenn.edu) \n 10 \nAbstract. Managing uncertainty is fundamental to geoscience practice, yet geoscience education generally does not \nincorporate explicit instruction on uncertainty. To the extent that students are exposed to scientific uncertainty, it is through \nin-person field experiences. Virtual field experiences – which rely on pictures, maps, and previously collected measurements \n– should therefore explicitly address uncertainty or risk losing this critical aspect of students’ experience. In this paper we \npresent a framework for teaching students to assess and communicate their uncertainty, which is grounded in best expert 15 \npractices for conveying uncertainty and familiar terms-of-art in geology. The starting point of our framework is the recognition \nof uncertainty in both geologic data and models, the latter of which we use as an encompassing term to refer to potential \ngeological processes and structures inferred on the basis of incomplete information. We present a concrete application of the \nframework to geological mapping and discuss how it could enhance student learning in both traditional in-person and virtual \nexperiences. Our framework is extensible in that it can be applied to a variety of geologic features beyond those where 20 \nuncertainty is traditionally assessed, and can also be applied to geological subdisciplines. \n1 Introduction \nCapitalizing fully on scientific research requires understanding how much uncertainty surrounds it (Fischhoff and Davis, 2014; \nKirch, 2012). In many cases these uncertainties extend beyond those associated with the measurement of observable \nphenomena and objects. This is particularly true in geoscience, where high levels of uncertainty are the standard, rather than 25 \nthe special case (Bárdossy and Fodor, 2001; Frodeman, 1995). For example, most field-based geoscientists are familiar with \nthe limitations on observation imposed by incomplete exposure. Even in cases where exposure is exceptional or complete, \ngeometric and lithologic complexities manifested over a wide range of spatial scales may lead to distinctly different \ninterpretations by individual scientists. In this way, learning to cope with uncertainty in its many forms is a central component \nin the training of an expert geoscientist.  30 \n \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '2 \n \nExactly how this occurs – how the characterization, assessment, and conveying of uncertainty changes from the novice to \nexpert mind – is not well understood (Petcovic et al., 2009). In-part, this is because sources and types of uncertainty change \ndepending on the scientific question and methodology, and so learning to cope with uncertainty is inherently specialized \n(Fischhoff and Davis, 2014). But it is also because the subjective inferences geoscientists make when faced with uncertainty 35 \nhave generally been an implicit element of the data collection and interpretation process (Bond, 2015). It is perhaps therefore \nunsurprising that formal geoscience curriculum never (or very rarely) explicitly addresses uncertainty and best practices for \nhandling it. We posit that, for many students, field mapping is the first exposure to the underlying uncertainties inherent in \ngeologic data and processes. For example, Petcovic et al. (2009) report the following anecdote from a novice mapper:  \n“...it was hard for me to identify [rocks] in the field because of the Ward’s samples that were given in the lab, which 40 \nare perfect representations of perfect rocks and minerals. I can honestly say that absolutely none of them … come \nwith algae and weathering and erosion and anything else subject to the environment…[note: Ward’s Scientific is a \ncompany that commonly supplies samples for education]” \n \nThis anecdote exemplifies the common experience of geology students wrestling with uncertainty in the field. Geology field 45 \neducators will recognize that, while some students can be overwhelmed by uncertainty in observations, others can \nsystematically underestimate uncertainty in their preoccupation with determining the “right answer” to a problem or field \narea, rather than attempting to establish a plausible interpretation that is most consistent with the data and their relative \nuncertainties. In both cases, what students need is language to understand the nature of uncertainty in geology – a \ncombination of uncertainty due to imperfect information in the world, and uncertainty due to the human mind that observes 50 \nthe information. This need has only become more timely and relevant with the move to replace traditional field mapping \nwith virtual experiences, brought on by the COVID-19 pandemic. Virtual field activities that rely on pictures, maps, and \npreviously collected measurements run the risk of obscuring uncertainties that would otherwise have a prominent influence \non scientific interpretation (if experienced in-person). How reliably can one infer the 3D structure of an outcrop from a series \nof 2D images? How certain can one be about the location of a lithologic boundary from satellite imagery alone?  55 \n \nHere, we address this pressing need by presenting a framework for teaching students to characterize, assess, and convey \nuncertainty that is suitable for both traditional in-person and virtual field experiences. The starting point of our framework \nis recognizing that there is uncertainty in both geologic data and models of geologic processes. We use the term data to refer \nto any geologic information (quantitative or qualitative) relevant to scientific judgment. We use the term model as an 60 \nencompassing term to refer to potential geological processes and structures inferred on the basis of incomplete data: it \nincludes model as it is traditionally used, as an account of what happened in the past to yield the current state of the Earth, \nas well as the related terms of theory, hypotheses, inference, and conjecture.  The reader might object to having all of these \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '3 \n \nterms treated as the same, but we are not arguing that they mean the same thing, only that they all refer to speculations \nabout the Earth that are drawn out of observations. Further, the reader may note that an important distinction among the 65 \nterms is the degree of evidence in support of the speculation (i.e. a simple inference is likely supported by less positive data \nthan is a widely accepted theory).  This distinction is in part what we seek to make explicit for students and experts in the \nform of uncertainty.  \n \nThe difference between data and model uncertainty is a useful high level categorization that is applicable to other sciences, 70 \nand conceptually similar to other scientific uncertainty categorization schemes in the learning sciences literature (Costanza \nand Cromwell, 1992; Pickering, 1995; Metz, 2004; Manz, 2015). However, what are the “data”, and the resulting models, will \ndiffer widely across sciences and even within subdisciplines of geology. In the next section, we present a detailed uncertainty \nrating scale suitable for geologic mapping in traditional and virtual field exercises. An important feature of our rating scale is \nthat it allows for assessment of uncertainty in qualitative observations, which makes up a large portion of geologic data. To 75 \nthe extent that students are familiar with assessing uncertainty in science, they are likely to be exposed to the more statistical \napproach of taking multiple quantitative measurements and estimating uncertainty based on numerical variability  (Bárdossy \nand Fodor, 2001). Yet, under many field conditions this approach is simply not feasible due to resource and time constraints, \nor impossible due to the qualitative nature of the observation. \n2 Uncertainty rating scale 80 \nOur work builds on existing best practices for assessing and conveying uncertainty on geologic maps, where uncertainties in \nthe location of faults and contacts between rock types are notated using solid lines (denoting high certainty), dashed lines, \nor dotted lines (denoting high uncertainty; see Fig 1A). This blunt categorization scheme used for mapping offers a useful \nstarting point for thinking about how geoscientists codify their uncertainties in the underlying observations and models. \nHowever, that system has significant limitations, the first of which being its applicability to a relatively narrow range of 85 \nrelevant observations (e.g. map-linear contacts). There is also no standard for how to use the low and high uncertainty \ncategories except by implicit inference from community practice. Finally, the dynamic range is likely compressed by experts \nbeing unwilling to publish high uncertainty observations or inferences, and having only three categories does not allow the \ncommunity to distinguish gradations of uncertainty, which is likely to be particularly important for students. \n 90 \nTo address these limitations, we developed an explicit and extendable six-category ranking system (“uncertainty rating \nscale”) for conveying the uncertainty associated with observations and models, grounded in familiar terms of art in geology \n(see Fig. 1B). The categories are: No evidence, Permissive, Suggestive, Presumptive, Compelling, and Certain. “No evidence” \nand “Certain” are end members, because there is no variability within these categories. No evidence indicates there is no \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '4 \n \ninformation that constrains an observation or model in any way. Certain indicates that an observation is unambiguously 95 \naccurate. We note that one key difference in the uncertainty rating scale as applied to observations when compared with \nmodels is that models cannot be regarded as certain. The middle four categories – Permissive, Suggestive, Presumptive, \nCompelling – have a range of possible values. Permissive is the least certain form of evidence.  Permissive suggests that a \nparticular observation or model cannot be ruled out, but it is also not the only available solution.  Suggestive indicates that \nthere is positive evidence for a particular observation or inference, but that the evidence also allows the possibility for other 100 \ninferences.  Presumptive – presumed in the absence of further information – indicates that an observation or model is more \nlikely right than wrong. Compelling indicates that the evidence is strongly supportive of the observation or model, and is \nnecessarily based on a preponderance of positive evidence.   \n \n 105 \n \nFigure 1: (A) Fragment of a geologic map of the Mount Barcroft-Blanco Mountain Area of Eastern California by Ernst and Hall \n(1987) showing moderate uncertainty in contact between older alluvial deposits (yellow) and Sage Hen Quartz Monzonite \n(pink), represented by a dashed line. The map also shows a fault in the Sage Hen unit, represented by a solid line where the \nfault is directly observable, and a dotted line when uncertain. (B) Evidence meter that captures a larger dynamic range in 110 \nuncertainty than the three-fold categorization used in A. \n \nThe uncertainty rating scale is extensible in that it can be applied to observational data about geologic features beyond those \nwhere uncertainty is traditionally assessed (e.g. map-linear contacts). Through interdisciplinary fieldwork with geologists and \ncognitive scientists, we identified four critical properties of an outcrop that could vary in uncertainty: Attachedness, 115 \nLithological Correlation, 3D Geometry, and Kinematics. Attachedness is the determination of whether the rock exposed at \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '5 \n \nthe Earth’s surface is directly connected to the rocks below the surface at that location; for example, a geologist who sees a \nsmall rock in a field but can not see the base of the rock might think, “well it could be attached to bedrock,” might record \nthe observation as permissible. Lithological Correlation is the determination of whether a particular rock belongs to a known \nformation; for example, a geologist may observe some unique structures or fabrics locally within formation A at its type 120 \nlocality, and subsequently conclude that those same structures or fabrics are suggestive of the presence of formation A at a \nnew, uncategorized location. 3D Geometry is the determination of the internal features of an outcrop (such as the orientation \nof bedding or other fabric) or the contact(s) between distinct features/units/lithologies in a field area; for example, when \ngeologists measure the strike and dip of bedding at an outcrop, it is generally considered presumptive that the orientation is \nrepresentative of that present in the subsurface, at least for some reasonable distance around where the measurement was 125 \ntaken. Kinematics is the determination of the past movement of the rock, such as along a fault; for example, most geologists \nwould take the repetition of a diagnostic sequence of well-exposed stratigraphy across a mapped fault as compelling \nevidence of shear sense. \n \nThe uncertainty rating scale was developed for simple observations and models/inferences, the type students are most likely 130 \nto focus on during field mapping, e.g., unit identification, the location of a fault or contact that isn’t visible in outcrops, sense \nof movement across a fault, etc. Using this example, a presumptive model  would be one that is more likely correct than not \nby virtue of being consistent with some presumptive data (or a lot of suggestive data), and inconsistent with only some \nsuggestive or permissive data, where there is no other model that accounts for the same or more data. However, the rating \nscale could readily apply to much larger models or interpretations about how the Earth works, such as unresolved alternative 135 \nhypotheses for a region, e.g., flat slab versus hit-and-run mechanism for Laramide deformation in North America (Maxson \nand Tikoff, 1996; English and Johnston, 2004), gravitational sinking versus proto-subduction for the formation of early \ncontinents (Brown et al., 2020). \n \n3 How might the uncertainty rating scale enhance student learning in traditional and virtual field experiences? 140 \nBroadly, there are two means through which the new uncertainty rating scale could benefit students during virtual and inperson\nfield experiences: one is through the actual practice of explicitly characterizing, assessing, and conveying uncertainty, \nand the other is through the consumption of science where uncertainty is (or is not) communicated. For both, there is \ntheoretical evidence uncertainty might benefit students (novices) and experts alike (Larrick and Feiler, 2015). However, here \nwe focus on the particular advantages for students. 145 \n \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '6 \n \nIn field practice, the presented geologic mapping framework provides students with a language to explicitly communicate \nuncertainty that they already feel, and that is true of the world. In science, it is often necessary to admit provisionally some \nassumptions that are not well-established, and this is particularly true for students who are working with less disciplinary \nknowledge than experts. Having a framework for conveying their uncertainty could help relieve students from some of the 150 \npsychological distress that is characteristic of “not knowing”, or affirm the development of their scientific ability by making \nprogress (in the form of reduced uncertainty over time) more conspicuous. Our own experience with students in the field \nsuggests that they are more likely to focus on their uncertainty than ignore it, but for some, an explicit framework for \ncommunicating uncertainty would enforce remembering what is only supposition – preventing the forgetting of uncertainty. \nThere is a tendency of the mind to “rest on an assumption” when it appears to fit in with other knowledge, and to forget 155 \nthat it has not been proved (Beveridge, 1957; Cantor, 1991). Students (and experts) who do not record uncertainty, \ntherefore, risk a rose-colored glasses effect whereby past uncertainties are systematically underestimated to achieve \nconcordance or consistency. \n \nAll the potential benefits of the uncertainty framework for geologic mapping in the field are also relevant to virtual practice. 160 \nBut unlike in the field, where students are consistently confronted with uncertainty, virtual activities may obscure \nuncertainties that would be otherwise prominent; the worry is not that students will forget about initial uncertainties, but \nthat they will not recognize them in the first place. According to the cognitive science literature, structured workflows could \nhelp scientists to recognize uncertainties they might otherwise overlook by explicitly enforcing the consideration of ideas \nthat would not have been fully deliberated otherwise (Soll et al., 2016; Wilson et al., 2019). For example, Macrae et al. (2016) 165 \nasked expert geoscientists to interpret a 2-D seismic reflection image and found that although experts reported that they \nwere considering geological history when making interpretations using their familiar workflows, only experts in a structured \nworkflow group showed evidence of effectively considering geological evolution of their interpretation. Extrapolating to the \nuncertainty framework, this suggests that students (and experts) might feel they have appropriately considered uncertainty, \nbut unless uncertainty assessment is made explicit through a workflow, there is a risk of their mismanaging and 170 \ninappropriately weighing uncertainty. \n \nWhen it comes to consuming science, having access to the uncertainties associated with the practitioners’ data and models \ncan help students’ fill-in missing context. This is perhaps most critical for consumption of science in virtual field experiences, \nwhere uncertainty judgments about attachedness, lithological correlation, 3D geometry, or kinematics can reveal features 175 \nof an environment that otherwise could not be perceived from available imagery.  Most importantly, however, having access \nto others’ uncertainty will make explicit that disagreements in uncertainty exist, and that these disagreements can be \nresolved in different ways to yield different geologic predictions. We believe that this will promote a more sophisticated \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '7 \n \ndiscourse amongst student teams and support the consideration of multiple working hypotheses. In the next section, we \npresent an initial framework for describing the relationship between data and model uncertainty to students. 180 \n4 Uncertainty in data and the models they produce \nStudents, by virtue of having less knowledge and experience than experts, will likely experience greater uncertainty in making \nobservations and collecting data. Students’ uncertainty in data will necessarily feed forward into their model uncertainty, as \nshown in Figure 2, because a model is only as good (certain) as the data that informs it. Inferences based on uncertain data \nmay increase the likelihood of bias in model selection – in the absence of certain data, a scientist is more likely to rely on 185 \nwhatever model is most accessible in their mind, perhaps what they worked on most frequently, most recently, or what is \nmost salient. This tendency to default to the interpretation that is most dominant in mind is referred to as the “availability \nheuristic” (Tversky and Kahneman, 1973), and it can lead to bias in experts (Bond et al., 2007) and students (Alcalde et al., \n2017).  For example, Alcalde et al. (2017) found that geology students were more likely to interpret a fault in a seismic image \nas normal and planar because this geometry was overrepresented in teaching materials the students encountered; after 190 \nstudents were exposed to a greater range of geometries through training, the range of interpretations increased. In short, \none rationale for recording and communicating uncertainty is that doing so can reduce bias (Macrae et al., 2016). \n \nThe relationship between data and model uncertainty is also bi-directional, in that model uncertainty also feeds backward \ninto data uncertainty (see Fig 2). Predictions based on uncertain (potentially biased) models guide what new data to collect, 195 \nbut can also lead to re-evaluation of previously collected data and their related uncertainty. Depending on how accurate the \nselected model is, either at predicting new data or accounting for previous data, the certainty of the model will also change. \nThis conception is, in practice, why the concept of multiple working hypotheses is so important within science (Chamberlin, \n1890). If left in isolation, a single preferred hypothesis/model is more likely to serve as its own reinforcement than it is to \nprovide a basis for future discoveries. 200 \n \n5 The way forward \nIn this paper we have provided a workflow for recording and communicating uncertainty for basic mapping.  We have \ngrounded the effort in principles of geology and the role of the mind in the practice of science.  The nature of uncertainty \nassociated with primary observations relevant to the subdomains of geoscience are likely broad and best described by 205 \nexperts.  Developing a system for those specific observations will likely require a community effort. Our informal impression \nfrom discussion with sub-domain experts is that the existence of uncertainty and the value in capturing it is immediately \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '8 \n \nrecognized, but exactly how that should be done and for what properties is not always clear.  Thus, in this section we briefly \nreview our process for coming to the properties reported, in the hope that others can use it to develop a workflow by analogy. \n 210 \nFigure 2: Bi-directional influence of data and model uncertainty. \n \nOur basic principles, which we suspect will be broadly applicable, are 1) uncertainty in any observation can be categorized \ninto a scale that ranges from completely uncertain to completely certain, and 2) uncertainty of different properties should \nbe orthogonal as much as possible, i.e., uncertainty in one property should not correlate with another.  Wherever uncertainty 215 \nis highly correlated across properties, they are not independent and thus are candidates for being collapsed into a single \nproperty. We applied these principles in the field while mapping the Sage Hen pluton and surrounding rocks in the WhiteInyo\nmountains of California.  We choose this area specifically because there exist two different published maps that differ \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '9 \n \nmarkedly in their interpretations of the area, reasoning that if experts had come to significantly different conclusions from \nsimilar observations it was likely the underlying data had significant levels of uncertainty. As we mapped at each stop we 220 \nasked, “what properties would be important to record here, and how certain are we that what we have recorded reflects \nthe true state of the world?” After the first day of mapping we asked, “are there any properties we have not recorded but \nwould record if we found evidence for?” These questions relatively quickly established the basic properties: lithology (“what \nare the rocks?”), attachedness (“are these rocks in-place?”), 3D geometry (“how are the rocks or contacts oriented?”), and \nkinematics (“do the rocks show evidence of movement?”).  The subsequent challenge was grounding the categories within 225 \neach property.  A discussion at an outcrop was often required to come to agreement about a category.  Experts began by \nprivately assessing uncertainty and then sharing to see how well they agreed and to discuss the origin of any disagreements, \nif there were any.  We observed that a useful touchstone for coming to consensus was the mid-point.  Here experts could \ngenerally agree about whether a given observation allowed one to determine if a property was more or less likely than all \nother possible interpretations. 230 \n \nWe recognize two unresolved questions from our work. The first is whether some categories should be interpreted as \nreflecting a quantitative range of spatial uncertainty.  For example, perhaps mapping a contact as suggestive should imply \nthat there was a spatial range for its location (e.g., +-100 meters)? The second is what the best way would be to establish \nthe scales so that the community can use them in the same way. During our small-group trip, we mutually adjusted to using 235 \nthe scale similarly after a few days such that there was high agreement on independent judgments of uncertainty. Likely a \nlifetime of experiences with the range of outcrop quality offered a common ground to scale uncertainty. The role of \nexperience in grounding the scale highlights the role of the academy in developing a profession that can adapt this framework \nfor capturing and communicating uncertainty. \n 240 \nWe might also ask how should students be taught to use an uncertainty scale in a way that maximizes the consistency of the \nuse of the scale and thus consistent application of the scale across users?  We propose an approach that has two parts, \nrecognizing that there is not a purely objective uncertainty for any given observation.  Rather, the uncertainty is a \ncombination of the skill and background of the observer and the rocks they are presented with. To educate students about \nthe uncertainty that can be attributed to the rocks we employed a strategy of showing pictures of outcrops accompanied by 245 \nan expert’s rating of uncertainty and their reasoning for that rating (these are available in the materials described in section \n7). Further, students would need practice introspecting about how uncertain they feel about the decisions they are making.  \nBy developing a practical pedagogy that can tune observers’ use of the scale, the community will have a mechanism for \naggregating trustworthy data. \n 250 \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '10 \n \n6 Conclusions \nIn this contribution, we address the current gap in undergraduate geoscience education on uncertainty.  We present a \nframework for teaching students to characterize, assess, and convey uncertainty that is suitable for both traditional in-person \nand virtual field experiences.  The 6-fold categories for data uncertainty are: No evidence, suggestive, presumptive, \ncompelling and certain.  For model uncertainty, the same ranking applies except that there are no “no evidence” and 255 \n“certain” characterizations.  We hope geoscience educators find the framework relevant and useful; to aid in its adoption, \nwe have provided materials for teaching about data and model uncertainty (described below).  The materials were designed \nfor basic geologic mapping education, but can be easily adapted, as discussed in the previous section.  \n \nThe benefits of the uncertainty framework for students is that it: (i) provides relief from the distress of “not knowing” by 260 \nproviding a language to communicate uncertainty; (ii) affirms scientific growth by making reductions in uncertainty more \nconspicuous; (iii) prevents systematic underestimation of uncertainty in field practice, and a failure to recognize uncertainty \nin virtual practice, by enforcing explicit consideration; (iv) allows access to others’ uncertainty fills-in missing context that \notherwise could not be perceived in virtual activities; and (v) makes the nuances of geologic interpretation more apparent \nby demonstrating that disagreements in uncertainty exist, and can be resolved in different ways, to produce different 265 \ngeologic predictions. In addition to these, we posit there are longer-term benefits from exposure to the uncertainty \nframework that will continue to serve students after the virtual or in-person field experience has ended. For students \npursuing careers in geology, for whom managing uncertainty will be fundamental, the framework can provide a critical \nscaffold on which new uncertainties can be built. Yet, even for those students who do not continue in science (which is nearly \nhalf; National Science Board, 2018; Chen, 2013), the uncertainty framework is still beneficial because it has the potential to 270 \nmove students from a black-and-white worldview (e.g., an interpretation is either right or wrong) to one of more nuance, \nwhere data has varying quality and interpretations have different strengths and weaknesses. We believe that this is the \nessential first step toward acceptance that published work is not “fact”, even if published by scientists with strong \nreputations. This kind of appreciation of the presence of uncertainty in science is a key insight for calibrating public trust in \nscience (Steijaert et al., 2020). 275 \n \n7 Materials for teaching uncertainty in geologic mapping \nWe provide materials for teaching about data and model uncertainty in the form of a virtual field-camp / capstone activity \nexercise developed (in part) by the authors. This exercise includes lecture and reading material, Google Earth referenced \ndata files and images, and student assignments structure on a day-by-day basis. The module is intended to be conducted 280 \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '11 \n \nover a period of 5-6 days. All materials are freely available from the Science Education Resource Center \n(https://serc.carleton.edu/NAGTWorkshops/online_field/activities/238026.html). \n \n8 Code availability \nNot applicable 285 \n9 Data availability \nNot applicable \n \n10 Executable research compendium \nNot applicable 290 \n \n11 Sample availability \nNot applicable \n \n12 Video supplement 295 \nNot applicable \n \n13 Supplement link \nhttps://serc.carleton.edu/NAGTWorkshops/online_field/activities/238026.html \n 300 \n13 Team list \nNot applicable \n \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', '12 \n \n15 Author contributions \nCGW, RTW, TFS, and BT developed the uncertainty rating scale and associated categorizations during field outings. CGW 305 \nprepared the manuscript with contributions from all co-authors \n \n16 Competing interests \nThe authors declare that they have no conflict of interest \n 310 \n17 Disclaimer \nThis research has been supported by the National Science Foundation, Future of Work at the Human Technology Frontier \n(grant no. 1839705) \n \n 315 \n18 References \nAlcalde, J., Bond, C. E., Johnson, G., Butler, R. W. H., Cooper, M. A., and Ellis, J. F.: The importance of structural model \navailability on seismic interpretation, J. Struct. Geol., 97, 161–171, https://doi.org/10.1016/j.jsg.2017.03.003, 2017 \nBárdossy, G. and Fodor, J.: Traditional and new ways to handle uncertainty in geology, Natural Resources Research, 10, 179–\n187, https://doi.org/10.1023/A:1012513107364, 2001. 320 \nBeveridge, W. I. B: The art of scientific investigation. W.W. Norton & Company Inc, New York, NY. 1957. \nBond, C. E.: Uncertainty in structural interpretation: Lessons to be learnt, J. Struct. Geol., 74, 185–200, \nhttps://doi.org/10.1016/j.jsg.2015.03.003, 2015. \nBond, C. E., Gibbs, A. D., Shipton, Z. K., and Jones, S.: What do you think this is? “Conceptual uncertainty” in geoscience \ninterpretation, Geological Society of America Today, 17, 4–10, https://doi.org/10.1130/GSAT01711A.1, 2007. 325 \nBrown, M., Johnson, T., and Gardiner, N. J.: Plate Tectonics and the Archean Earth. Annual Review of Earth and Planetary \nSciences, 48, 291-320, https://doi.org/10.1146/annurev-earth-081619-052705, 2020. \nCantor, G.: Faraday on Scientific Method, in: Michael Faraday: Sandemanian and Scientist. Palgrave Macmillan, London, 196124,\n1991. \nCostanza, R., & Cornwell, L.: The 4P approach to dealing with scientific uncertainty. Environment: Science and Policy for 330 \nSustainable Development, 34, 12-42, https://doi.org/10.1080/00139157.1992.9930930, 1992. \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n', "13 \n \nEnglish, J. M. and Johnston, S. T.: The Laramide orogeny: What were the driving forces?. International Geology Review, 46, \n833-838, https://doi.org/10.2747/0020-6814.46.9.833, 2004. \nFischhoff, B., & Davis, A. L.: Communicating scientific uncertainty. Proceedings of the National Academy of Sciences, 111, \n13664-13671, https://doi.org/10.1073/pnas.1317504111, 2014. 335 \nFrodeman, R.: Geological reasoning: Geology as an interpretive and historical science, Geol. Soc. Am. Bull., 107, 960–968, \nhttps://doi.org/10.1130/0016-7606(1995)107<0960:GRGAAI>2.3.CO;2, 1995. \nKirch, S. A.: Understanding scientific uncertainty as a teaching and learning goal, in: Second international handbook of science \neducation, edited by: Fraser, B., Tobin, K., and McRobbie, C., Springer Springer,  New York, NY, 851-864, \nhttps://doi.org/10.1007/978-1-4020-9041-7_57, 2012. 340 \nLarrick, R.P. and Feiler, D.C. Expertise in decision making, in: The Wiley Blackwell Handbook of Judgment and Decision \nMaking, 1st Edn., edited by: Keren, G. and Wu. G, John Wiley & Sons Ltd, Hoboken, NJ, 696-721, 2015. \nManz, E.: Representing student argumentation as functionally emergent from scientific activity. Review of Educational \nResearch, 85, 553-590, https://doi.org/10.3102/0034654314558490, 2015. \nMaxson, J. and Tikoff, B.: Hit-and-run collision model for the Laramide orogeny, western United States. Geology, 24, 968-345 \n972, https://doi.org/10.1130/0091-7613(1996)024%3C0968:HARCMF%3E2.3.CO;2,  1996. \nMetz, K. E.: Children's understanding of scientific inquiry: Their conceptualization of uncertainty in investigations of their \nown design. Cognition and Instruction, 22, 219-290, https://doi.org/10.1207/s1532690xci2202_3, 2004. \nNational Science Board: Science and Engineering Indicators 2018. Alexandria, VA: National Science Foundation, NSB-2018-1, \n2018. 350 \nPetcovic, H.L., Libarkin, J.C. and Baker, K.M.: An empirical methodology for investigating geocognition in the field. Journal of \nGeoscience Education, 57, 316-328, https://doi.org/10.5408/1.3544284, 2009. \nPickering, A.: Cyborg history and the World War II regime. Perspectives on Science, 3, 1-48, 1995. \nSoll, J. B., Milkman, K. L., and Payne, J. W.: A user's guide to debiasing, in: Wiley Blackwell Handbook of Judgment and Decision \nMaking, 2nd Edn., edited by: Keren, G. and Wu, G., Blackwell Publishing Ltd, Hoboken, NJ, 506–951, 2016 355 \nSteijaert, M. J., Schaap, G. and Riet, J. V.: Two-sided science: Communicating scientific uncertainty increases trust in scientists \nand donation intention by decreasing attribution of communicator bias, Communications, \nhttps://doi.org/10.1515/commun-2019-0123, 2020. \nTversky, A., & Kahneman, D.: Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 5, 207-232, \nhttps://doi.org/10.1016/0010-0285(73)90033-9, 1973. 360 \nChen, X.: STEM Attrition: College Students' Paths into and out of STEM Fields, Statistical Analysis Report, NCES 2014-001, \nNational Center for Education Statistics, 2013. \n \nhttps://doi.org/10.5194/se-2021-69\nPreprint. Discussion started: 21 May 2021\nc© Author(s) 2021. CC BY 4.0 License.\n"]</pre>
    </body>
    </html>
    